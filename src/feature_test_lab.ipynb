{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Explored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import math\n",
    "import pickle\n",
    "import gensim\n",
    "from textblob import Sentence\n",
    "\n",
    "delimiter = '\\t'\n",
    "dir_name = '/Users/nbarnaba/PycharmProjects/Keras_Samples/src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tweets_as_whole_text(df):\n",
    "    texts = df[1].values\n",
    "    whole_text = ''\n",
    "    for each in texts:\n",
    "        whole_text = whole_text + ' ' + each\n",
    "    return whole_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = os.path.join(dir_name, '..', 'data', 'en_train', 'EI-reg-en_anger_train.txt')\n",
    "df_anger = pd.read_csv(train_file_name, header=None, delimiter=delimiter)\n",
    "anger_texts = get_all_tweets_as_whole_text(df_anger)\n",
    "anger_hashtags = [each[0] for each in Counter(re.findall('#\\w+', anger_texts)).most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = os.path.join(dir_name, '..', 'data', 'en_train',  'EI-reg-en_fear_train.txt')\n",
    "df_fear = pd.read_csv(train_file_name, header=None, delimiter=delimiter)\n",
    "fear_texts = get_all_tweets_as_whole_text(df_fear)\n",
    "fear_hashtags = [each[0] for each in Counter(re.findall('#\\w+', fear_texts)).most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = os.path.join(dir_name, '..', 'data', 'en_train',  'EI-reg-en_sadness_train.txt')\n",
    "df_sadness = pd.read_csv(train_file_name, header=None, delimiter=delimiter)\n",
    "sadness_texts = get_all_tweets_as_whole_text(df_sadness)\n",
    "sadness_hashtags = [each[0] for each in Counter(re.findall('#\\w+', sadness_texts)).most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = os.path.join(dir_name, '..', 'data', 'en_train',  'EI-reg-en_joy_train.txt')\n",
    "df_joy = pd.read_csv(train_file_name, header=None, delimiter=delimiter)\n",
    "joy_texts = get_all_tweets_as_whole_text(df_joy)\n",
    "joy_hashtags = [each[0] for each in Counter(re.findall('#\\w+', joy_texts)).most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = anger_hashtags + joy_hashtags + sadness_hashtags + fear_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_hashtags = [each[0] for each in Counter(hashtags).most_common() if each[1]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anger : 483\n",
      "fear : 963\n",
      "joy : 529\n",
      "sadness : 582\n",
      "unique : 2557\n",
      "total : 3363\n"
     ]
    }
   ],
   "source": [
    "anger_hashtags = [each for each in anger_hashtags if each in unique_hashtags]\n",
    "fear_hashtags = [each for each in fear_hashtags if each in unique_hashtags]\n",
    "joy_hashtags = [each for each in joy_hashtags if each in unique_hashtags]\n",
    "sadness_hashtags = [each for each in sadness_hashtags if each in unique_hashtags]\n",
    "print('Anger : '+str(len(anger_hashtags)))\n",
    "print('fear : '+str(len(fear_hashtags)))\n",
    "print('joy : '+str(len(joy_hashtags)))\n",
    "print('sadness : '+str(len(sadness_hashtags)))\n",
    "print('unique : '+str(len(unique_hashtags)))\n",
    "print('total : '+str(len(hashtags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode Emo Hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1371909298\n",
      "          0                 1         2\n",
      "3908   fear         #westbank  1.951954\n",
      "3909   fear     #apprehension  1.951954\n",
      "3910   fear            #su4mh  1.951954\n",
      "3911   fear          aaaaaaah  1.951954\n",
      "3912   fear              #ied  1.951954\n",
      "3913   fear        #coldsweat  1.951954\n",
      "3914   fear             #isaf  1.951954\n",
      "3915   fear           cryotek  1.951954\n",
      "3916   fear          #rushing  1.951954\n",
      "3917   fear              #shy  1.951954\n",
      "3918   fear     #apprehensive  1.951954\n",
      "3919   fear            #mosul  1.951954\n",
      "3920   fear          #fearful  1.951954\n",
      "3921   fear           #ashdod  1.951954\n",
      "3922   fear    #socialanxiety  1.951954\n",
      "3923   fear        #backtrack  1.951954\n",
      "3924   fear         terrifies  1.951954\n",
      "3925   fear   #claustrophobia  1.951954\n",
      "3926   fear            qassam  1.951954\n",
      "3927   fear        #hezbollah  1.951954\n",
      "3928   fear          #hamas25  1.951954\n",
      "3929   fear              #dfw  1.951954\n",
      "3930   fear           #feared  1.951954\n",
      "3931   fear          #taliban  1.951954\n",
      "3932   fear       rationality  1.951954\n",
      "3933   fear  #israelunderfire  1.951954\n",
      "3934   fear     #ihatespiders  1.951954\n",
      "3935   fear         #pharmacy  1.951954\n",
      "3936   fear              #pij  1.951954\n",
      "3937   fear          #conquer  1.951954\n",
      "...     ...               ...       ...\n",
      "26997   joy           killers  0.008800\n",
      "26998   joy             psych  0.008800\n",
      "26999   joy           atlanta  0.008800\n",
      "27000   joy              laid  0.008800\n",
      "27001   joy            papers  0.008800\n",
      "27002   joy          showered  0.008800\n",
      "27003   joy              till  0.008547\n",
      "27004   joy             faces  0.006399\n",
      "27005   joy                24  0.006239\n",
      "27006   joy         finishing  0.006151\n",
      "27007   joy             ghost  0.005845\n",
      "27008   joy         therefore  0.005845\n",
      "27009   joy             films  0.005461\n",
      "27010   joy          survived  0.004961\n",
      "27011   joy            quotes  0.004961\n",
      "27012   joy               toy  0.004285\n",
      "27013   joy              safe  0.003569\n",
      "27014   joy        wallflower  0.003320\n",
      "27015   joy          monsters  0.003320\n",
      "27016   joy             frame  0.003320\n",
      "27017   joy          romantic  0.003320\n",
      "27018   joy           signing  0.003320\n",
      "27019   joy             pulls  0.003320\n",
      "27020   joy               100  0.003320\n",
      "27021   joy               dvd  0.003320\n",
      "27022   joy            sweets  0.003320\n",
      "27023   joy             apart  0.002558\n",
      "27024   joy            doctor  0.002035\n",
      "27025   joy              grad  0.001137\n",
      "27026   joy              1000  0.001137\n",
      "\n",
      "[15421 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "file_name = os.path.join(dir_name, '..','resources', 'NRC-Hashtag-Emotion-Lexicon-v0.2', 'NRC-Hashtag-Emotion-Lexicon-v0.2.txt')\n",
    "df_emo_hashtag = pd.read_csv(file_name, header=None, delimiter=delimiter)\n",
    "df_emo_hashtag = df_emo_hashtag[(df_emo_hashtag[0] == 'sadness') |\n",
    "                               (df_emo_hashtag[0] == 'anger') | \n",
    "                               (df_emo_hashtag[0] == 'fear') | \n",
    "                               (df_emo_hashtag[0] == 'joy')]\n",
    "print(df_emo_hashtag[(df_emo_hashtag[0] == 'anger') & (df_emo_hashtag[1] == 'pissed')][2].values[0])\n",
    "print(df_emo_hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['fear', '#westbank', 1.95195360556393],\n",
       "       ['fear', '#apprehension', 1.95195360556393],\n",
       "       ['fear', '#su4mh', 1.95195360556393],\n",
       "       ..., \n",
       "       ['joy', 'doctor', 0.0020351827030041197],\n",
       "       ['joy', 'grad', 0.00113671004597709],\n",
       "       ['joy', '1000', 0.00113671004597709]], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_hashtag = df_emo_hashtag.as_matrix()\n",
    "emo_hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy', 'grad', 0.00113671004597709], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_emo_hashtag(emotion, hashtag):\n",
    "    temp = None\n",
    "    for each in emo_hashtag: \n",
    "        if each[0] == emotion and (each[1] == hashtag):\n",
    "            if temp is None:\n",
    "                temp = each\n",
    "            else:\n",
    "                temp = np.concatenate((temp, each), axis=0)\n",
    "    return temp\n",
    "filter_emo_hashtag('joy', 'grad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(tweet_content):\n",
    "    return re.findall('#[a-zA-Z]+', tweet_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtag_emo_value(hashtag, emotion):\n",
    "    emo_value = filter_emo_hashtag(emotion, hashtag)\n",
    "    return emo_value[2] if emo_value is not None else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emot_value_from_hashtag(tweet_content, emotion):\n",
    "    hashtags = get_hashtags(tweet_content)\n",
    "    if len(hashtags) > 0:\n",
    "        _t_emo = []\n",
    "        for each_hashtag in hashtags:\n",
    "            _t_emo.append(get_hashtag_emo_value(each_hashtag, emotion))\n",
    "    else:\n",
    "        _t_emo = [0]\n",
    "    return np.mean(_t_emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger_hashtag_feature_nrc_hashtag_emoticon = np.array([get_emot_value_from_hashtag(each, 'anger') \n",
    "                                                       for each in df_anger.as_matrix()[:,1]]).reshape(len(df_anger), 1)\n",
    "joy_hashtag_feature_nrc_hashtag_emoticon = np.array([get_emot_value_from_hashtag(each, 'joy') \n",
    "                                                     for each in df_joy.as_matrix()[:,1]]).reshape(len(df_joy), 1)\n",
    "fear_hashtag_feature_nrc_hashtag_emoticon = np.array([get_emot_value_from_hashtag(each, 'fear')\n",
    "                                                      for each in df_fear.as_matrix()[:,1]]).reshape(len(df_fear), 1)\n",
    "sadness_hashtag_feature_nrc_hashtag_emoticon = np.array([get_emot_value_from_hashtag(each, 'sadness') \n",
    "                                                         for each in df_sadness.as_matrix()[:,1]]).reshape(len(df_sadness), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1701, 1)\n",
      "(1616, 1)\n",
      "(2252, 1)\n",
      "(1533, 1)\n"
     ]
    }
   ],
   "source": [
    "print(anger_hashtag_feature_nrc_hashtag_emoticon.shape)\n",
    "print(joy_hashtag_feature_nrc_hashtag_emoticon.shape)\n",
    "print(fear_hashtag_feature_nrc_hashtag_emoticon.shape)\n",
    "print(sadness_hashtag_feature_nrc_hashtag_emoticon.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Unicode Emoticon Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "translation_table = dict.fromkeys(map(ord, string.punctuation), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['jeffreydonovan' 5.0 6 0]\n",
      " ['familar' 5.0 6 0]\n",
      " ['vppatel2011' 5.0 6 0]\n",
      " ..., \n",
      " ['clarianne' -4.999 0 5]\n",
      " ['scrambling' -4.999 0 8]\n",
      " ['ballsed' -4.999 0 6]]\n"
     ]
    }
   ],
   "source": [
    "file_name = os.path.join(dir_name, '..','resources', 'NRC-Emoticon-Lexicon-v1.0', 'Emoticon-unigrams.txt')\n",
    "df_emo_words = pd.read_csv(file_name, header=None, delimiter=delimiter)\n",
    "df_emo_words[0] = df_emo_words[0].str.lower().str.translate(translation_table)\n",
    "emo_words = df_emo_words.as_matrix() \n",
    "print(emo_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.999\n"
     ]
    }
   ],
   "source": [
    "def filter_emo_word(word):\n",
    "    temp = None\n",
    "    for each in emo_words: \n",
    "        if each[0] == word:\n",
    "            if temp is None:\n",
    "                temp = each\n",
    "            else:\n",
    "                temp = np.concatenate((temp, each), axis=0)\n",
    "    return temp\n",
    "print (filter_emo_word('ballsed')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emoticon_lexicon_value(df):\n",
    "    translation_table = dict.fromkeys(map(ord, string.punctuation), None)\n",
    "    cleaned_tokenized = df[1].str.lower().str.translate(translation_table).str.split().values\n",
    "    emoticon_lexicon_value = []\n",
    "    for each_tweet in cleaned_tokenized:\n",
    "        emoti_word_count = 0\n",
    "        emoti_value = 0\n",
    "        for each_word in each_tweet:\n",
    "\n",
    "            tmp = filter_emo_word(each_word)\n",
    "            if tmp is not None:\n",
    "                emoti_value += tmp[1]\n",
    "                emoti_word_count += 1\n",
    "        emoti_value = (emoti_value)/(emoti_word_count+1)\n",
    "        emoticon_lexicon_value.append(emoti_value)\n",
    "    return emoticon_lexicon_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger_emoticon_lexicon_value = get_emoticon_lexicon_value(df_anger)\n",
    "joy_emoticon_lexicon_value = get_emoticon_lexicon_value(df_joy)\n",
    "sadness_emoticon_lexicon_value = get_emoticon_lexicon_value(df_sadness)\n",
    "fear_emoticon_lexicon_value = get_emoticon_lexicon_value(df_fear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joy_emoticon_lexicon_value = np.array(joy_emoticon_lexicon_value).reshape(len(joy_emoticon_lexicon_value),1)\n",
    "anger_emoticon_lexicon_value = np.array(anger_emoticon_lexicon_value).reshape(len(anger_emoticon_lexicon_value),1)\n",
    "fear_emoticon_lexicon_value = np.array(fear_emoticon_lexicon_value).reshape(len(fear_emoticon_lexicon_value),1)\n",
    "sadness_emoticon_lexicon_value = np.array(sadness_emoticon_lexicon_value).reshape(len(sadness_emoticon_lexicon_value),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Vector (by averaging the constituent word vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_vectors():\n",
    "    global word_model\n",
    "    global dir_name\n",
    "#     embedding_file_loc = os.path.join(dir_name, '..', 'resources', 'GoogleNews-vectors-negative300.bin')\n",
    "    embedding_file_loc = os.path.join(dir_name, '..', 'resources', 'wiki.en', 'wiki.en.vec')\n",
    "    print(\"Loading the data file... Please wait...\")\n",
    "    word_model = gensim.models.KeyedVectors.load_word2vec_format(embedding_file_loc, binary=False)\n",
    "    print(\"Successfully loaded 3.6 G bin file!\")\n",
    "    return word_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase_vector_obj(value):\n",
    "    return PhraseVector(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhraseVector:\n",
    "    def __init__(self, phrase):\n",
    "        self.phrase = phrase\n",
    "        self.vector = self.phrase_to_vec(phrase)\n",
    "        self.pos_tag = self.get_words_in_phrase(phrase)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_vector_set_to_average(vector_set, ignore=[]):\n",
    "        if len(ignore) == 0:\n",
    "            return np.mean(vector_set, axis=0)\n",
    "        else:\n",
    "            return np.dot(np.transpose(vector_set), ignore) / sum(ignore)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_unique_token_tags(vector1, vector2):\n",
    "        tag_list = []\n",
    "        for each_tag in vector1.pos_tag + vector2.pos_tag:\n",
    "            if each_tag not in tag_list:\n",
    "                tag_list.append(each_tag)\n",
    "        return tag_list\n",
    "\n",
    "    def phrase_to_vec(self, phrase):\n",
    "        # _stop_words = stopwords.words(\"english\")\n",
    "        phrase = phrase.lower()\n",
    "        verified_words = [word for word in phrase.split()]\n",
    "        vector_set = []\n",
    "        for each_word in verified_words:\n",
    "            try:\n",
    "                word_vector = word_model[each_word]\n",
    "                vector_set.append(word_vector)\n",
    "            except:\n",
    "                pass\n",
    "        return self.convert_vector_set_to_average(vector_set)\n",
    "\n",
    "    def get_cosine_similarity(self, other_vector):\n",
    "        cosine_similarity = np.dot(self.vector, other_vector.vector) / (\n",
    "        np.linalg.norm(self.vector) * np.linalg.norm(other_vector.vector))\n",
    "        try:\n",
    "            if math.isnan(cosine_similarity):\n",
    "                cosine_similarity = 0\n",
    "        except:\n",
    "            cosine_similarity = 0\n",
    "        return cosine_similarity\n",
    "\n",
    "    def get_words_in_phrase(self, phrase):\n",
    "        if phrase.strip() == '':\n",
    "            return []\n",
    "        else:\n",
    "            tagged_input = nltk.pos_tag(phrase.split(), tagset='universal')\n",
    "            prev_item, prev_tag = tagged_input[0]\n",
    "            g_item_list = [prev_item]\n",
    "            cur_group_index = 0\n",
    "            space = ' '\n",
    "            revised_tag = []\n",
    "            for cur_item, cur_tag in tagged_input[1:]:\n",
    "                cur_item = cur_item.lower()\n",
    "                if prev_tag is cur_tag:\n",
    "                    g_item_list[cur_group_index] += space + cur_item\n",
    "                else:\n",
    "                    revised_tag.append((g_item_list[cur_group_index], prev_tag))\n",
    "                    prev_tag = cur_tag\n",
    "                    g_item_list.append(cur_item)\n",
    "                    cur_group_index += 1\n",
    "            revised_tag.append((g_item_list[cur_group_index], prev_tag))\n",
    "            return revised_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data file... Please wait...\n",
      "Successfully loaded 3.6 G bin file!\n",
      "<gensim.models.keyedvectors.KeyedVectors object at 0x11ee77588>\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "word_model = load_word_vectors()\n",
    "print(word_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase_vectors(df, emotion):\n",
    "    tweet_vectors_obj = None\n",
    "    tweet_vectors = None\n",
    "    labels = None\n",
    "    filename = os.path.join(dir_name, '..', 'resources', 'raw_phrase_vectors_obj_'+emotion)\n",
    "    if not os.path.exists(filename):\n",
    "        tweet_vectors_obj = np.vectorize(get_phrase_vector_obj)(df[1].values)\n",
    "        tweet_vectors = np.array([[]])\n",
    "        for each_vector in tweet_vectors_obj:\n",
    "            curr_vector = each_vector.vector\n",
    "            if np.isnan(curr_vector).any():\n",
    "                curr_vector = np.zeros(shape=(1, 300))\n",
    "            else:\n",
    "                curr_vector = curr_vector.reshape(1, len(each_vector.vector))\n",
    "            if np.min(tweet_vectors.shape) == 0:\n",
    "                tweet_vectors = np.concatenate((tweet_vectors, curr_vector), axis=1)\n",
    "            else:\n",
    "                tweet_vectors = np.concatenate((tweet_vectors, curr_vector), axis=0)\n",
    "        labels = df[3].values\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(tweet_vectors_obj, f)\n",
    "            pickle.dump(tweet_vectors, f)\n",
    "            pickle.dump(labels, f)\n",
    "    else:\n",
    "        with open(filename, 'rb') as f:\n",
    "            tweet_vectors_obj = pickle.load(f)\n",
    "            tweet_vectors = pickle.load(f)\n",
    "            labels = pickle.load(f)\n",
    "    return tweet_vectors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_vectors_anger, labels_anger = get_phrase_vectors(df_anger, emotion='anger')\n",
    "tweet_vectors_joy, labels_joy = get_phrase_vectors(df_joy, emotion='joy')\n",
    "tweet_vectors_sadness, labels_sadness = get_phrase_vectors(df_sadness, emotion='sadness')\n",
    "tweet_vectors_fear, labels_fear = get_phrase_vectors(df_fear, emotion='fear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarity and Subjectivity using Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polarity_and_subjectivity(df, emotion):\n",
    "    polarity_list = []\n",
    "    subjectivity_list = []\n",
    "    filename = os.path.join(dir_name, '..', 'resources', 'polarity_and_subjectivity_'+emotion)\n",
    "    if not os.path.exists(filename):\n",
    "        polarity_list = np.array(list(map(lambda x: Sentence(x).polarity, df[1].values)))\n",
    "        subjectivity_list = np.array(list(map(lambda x: Sentence(x).subjectivity, df[1].values)))\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(polarity_list, f)\n",
    "            pickle.dump(subjectivity_list, f)\n",
    "    else:\n",
    "        with open(filename, 'rb') as f:\n",
    "            polarity_list = pickle.load(f)\n",
    "            subjectivity_list = pickle.load(f)\n",
    "    polarity_list = polarity_list.reshape(len(polarity_list),1)\n",
    "    subjectivity_list = subjectivity_list.reshape(len(subjectivity_list),1)\n",
    "    return polarity_list, subjectivity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_list_anger, subjectivity_list_anger = get_polarity_and_subjectivity(df_anger, emotion='anger')\n",
    "polarity_list_fear, subjectivity_list_fear = get_polarity_and_subjectivity(df_fear, emotion='fear')\n",
    "polarity_list_sadness, subjectivity_list_sadness = get_polarity_and_subjectivity(df_sadness, emotion='sadness')\n",
    "polarity_list_joy, subjectivity_list_joy = get_polarity_and_subjectivity(df_joy, emotion='joy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Features Concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger_features = np.concatenate((polarity_list_anger, \n",
    "                                 subjectivity_list_anger, \n",
    "                                 tweet_vectors_anger, \n",
    "                                 anger_emoticon_lexicon_value,\n",
    "                                 anger_hashtag_feature_nrc_hashtag_emoticon), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "fear_features = np.concatenate((polarity_list_fear, \n",
    "                                 subjectivity_list_fear, \n",
    "                                 tweet_vectors_fear, \n",
    "                                 fear_emoticon_lexicon_value,\n",
    "                                 fear_hashtag_feature_nrc_hashtag_emoticon), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "joy_features = np.concatenate((polarity_list_joy, \n",
    "                                 subjectivity_list_joy, \n",
    "                                 tweet_vectors_joy, \n",
    "                                 joy_emoticon_lexicon_value,\n",
    "                                 joy_hashtag_feature_nrc_hashtag_emoticon), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "sadness_features = np.concatenate((polarity_list_sadness, \n",
    "                                 subjectivity_list_sadness, \n",
    "                                 tweet_vectors_sadness, \n",
    "                                 sadness_emoticon_lexicon_value,\n",
    "                                 sadness_hashtag_feature_nrc_hashtag_emoticon), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1533, 303)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sadness_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer, maketrans\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.constraints import min_max_norm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "\n",
    "max_length = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_parameters(features):\n",
    "    vocab_size, embedded_vector_length = features.shape\n",
    "    max_length = 1 # One embedding vector per tweet\n",
    "    embedding_matrix = features\n",
    "    return vocab_size, embedded_vector_length, max_length, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_model(vocab_size, embedded_vector_length, embedding_matrix, max_length):\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size, embedded_vector_length, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "    model.add(e)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnn_model(embedding_models,\n",
    "                  optimizer='adam',\n",
    "                  loss='mean_squared_logarithmic_error',\n",
    "                  output_activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Merge(embedding_models, mode='concat'))\n",
    "    lstm = Bidirectional(LSTM(100, activation='relu',\n",
    "                              kernel_initializer='random_uniform',\n",
    "                              bias_initializer='zeros',\n",
    "                              kernel_constraint=min_max_norm(min_value=-1.0, max_value=1.0, rate=0.5, axis=0),\n",
    "                              bias_constraint=min_max_norm(min_value=-1.0, max_value=1.0, rate=0.5, axis=0)))\n",
    "    model.add(lstm)\n",
    "    model.add(Dense(1, activation=output_activation,\n",
    "                    kernel_initializer='random_uniform',\n",
    "                    bias_initializer='zeros',\n",
    "                    kernel_constraint=min_max_norm(min_value=-1.0, max_value=1.0, rate=.5, axis=0),\n",
    "                    bias_constraint=min_max_norm(min_value=-1.0, max_value=1.0, rate=0.5, axis=0)))\n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "    return model\n",
    "    # fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_index(embedding_file_name):\n",
    "    # load the whole embedding into memory\n",
    "    embeddings_index = dict()\n",
    "    # f = open('../resources/glove.6B/glove.6B.'+str(embedded_vector_length)+'d.txt')\n",
    "    with open(embedding_file_name) as f:\n",
    "        l_no = 0\n",
    "        for line in f:\n",
    "            if l_no == 0:\n",
    "                l_no += 1\n",
    "                continue\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            try:\n",
    "                coefs = asarray(values[1:], dtype='float32')\n",
    "            except ValueError as e:\n",
    "                coefs = asarray(values[-embedded_vector_length:], dtype='float32')\n",
    "            if word not in embeddings_index.keys():\n",
    "                embeddings_index[word] = coefs\n",
    "    return  embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_docs(docs):\n",
    "    global max_length\n",
    "    t = Tokenizer(char_level=False)\n",
    "    t.fit_on_texts(docs)\n",
    "    encoded_docs = t.texts_to_sequences(docs)\n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    return padded_docs, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_sequence(text,\n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                          lower=True, split=\" \"):\n",
    "    \"\"\"Converts a text to a sequence of words (or tokens).\n",
    "\n",
    "    # Arguments\n",
    "        text: Input text (string).\n",
    "        filters: Sequence of characters to filter out.\n",
    "        lower: Whether to convert the input to lowercase.\n",
    "        split: Sentence split marker (string).\n",
    "\n",
    "    # Returns\n",
    "        A list of words (or tokens).\n",
    "    \"\"\"\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "\n",
    "    if sys.version_info < (3,) and isinstance(text, bytes):\n",
    "        translate_map = dict((ord(c), bytes(split)) for c in filters)\n",
    "    else:\n",
    "        translate_map = maketrans(filters, split * len(filters))\n",
    "\n",
    "    text = text.translate(translate_map)\n",
    "    seq = text.split(split)\n",
    "    return [i for i in seq if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(tweet_ids, assgn_emotions, tweet_contents, predicted_scores, file_name):\n",
    "    dir_name = '/Users/nbarnaba/PycharmProjects/Keras_Samples/src'\n",
    "    if not os.path.exists(os.path.join(dir_name, '..', 'output')):\n",
    "        os.makedirs(os.path.join(dir_name, '..', 'output'))\n",
    "    with open(os.path.join(dir_name, '..', 'output', file_name), 'w') as f:\n",
    "        file_writer = csv.writer(f, delimiter='\\t')\n",
    "        for each_tweet_id, each_tweet_content, each_emotion, each_score in \\\n",
    "                zip(tweet_ids, tweet_contents, assgn_emotions, predicted_scores):\n",
    "            file_writer.writerow([each_tweet_id, each_tweet_content, each_emotion, each_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs_contents(file_name):\n",
    "    global max_length\n",
    "    docs = []\n",
    "    labels = []\n",
    "    tweet_ids = []\n",
    "    emotions = []\n",
    "    with open(file_name) as f:\n",
    "        for each_record in f:\n",
    "            record_tokens = each_record.split('\\t')\n",
    "            content = record_tokens[1].lower()\n",
    "            tweet_id = record_tokens[0]\n",
    "            current_emotion = record_tokens[2]\n",
    "            docs.append(content)\n",
    "            emotions.append(current_emotion)\n",
    "            tweet_ids.append(tweet_id)\n",
    "            seq = content if char_level else text_to_word_sequence(content, filters='')\n",
    "            temp = len([w for w in seq])\n",
    "            if temp > max_length:\n",
    "                max_length = temp\n",
    "            labels.append(float(record_tokens[3][:-1]))\n",
    "    return tweet_ids, docs, emotions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = '/Users/nbarnaba/PycharmProjects/Keras_Samples/src'\n",
    "char_level = False\n",
    "docs = []\n",
    "labels = []\n",
    "tweet_ids = []\n",
    "emotions = []\n",
    "\n",
    "embedded_vector_length = 300\n",
    "emotion_names = ['sadness', 'joy', 'anger', 'fear']\n",
    "embedding_name = 'wiki.en'\n",
    "embeddings_index = get_embedding_index(os.path.join(dir_name,\n",
    "                                                    '..',\n",
    "                                                    'resources',\n",
    "                                                    embedding_name,\n",
    "                                                    'wiki.en.vec'))\n",
    "                                                    # 'glove.6B.' + str(embedded_vector_length) + 'd.txt'))\n",
    "\n",
    "emot_features = [sadness_features, joy_features, anger_features, fear_features]\n",
    "for emot_id, emotion in enumerate(emotion_names):\n",
    "\n",
    "    tweet_ids_train, docs_train, emotions_train, label_train = get_docs_contents(os.path.join(dir_name, '..','data','en_train','EI-reg-en_'+emotion+'_train.txt'))\n",
    "\n",
    "#     tweet_ids, docs, emotions, labels = shuffle(tweet_ids, docs, emotions, labels)\n",
    "#     tweet_ids_train, tweet_ids_test, \\\n",
    "#     docs_train, doc_test, \\\n",
    "#     emotions_train, emotions_test, \\\n",
    "#     label_train, label_test = train_test_split(tweet_ids, docs, emotions, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "    padded_docs_train, t = get_padded_docs(docs_train)\n",
    "    vocab_size = len(t.word_index) + 1\n",
    "\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    embedding_matrix = zeros((vocab_size, embedded_vector_length))\n",
    "    for word, i in t.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    # define model with word embeddings\n",
    "    word_model = get_embedding_model(vocab_size, embedded_vector_length, embedding_matrix, max_length)\n",
    "    word_model.fit(padded_docs_train, label_train, verbose=1)\n",
    "    \n",
    "    # Added features\n",
    "    current_feature = emot_features[emot_id]\n",
    "    vocab_size, embedded_vector_length, max_length, embedding_matrix = get_embedding_parameters(current_feature)\n",
    "    padded_feat_train = np.arange(vocab_size).reshape(vocab_size, 1)\n",
    "    extra_features_model = get_embedding_model(vocab_size, embedded_vector_length, embedding_matrix, max_length)\n",
    "    \n",
    "    model  =get_rnn_model([padded_docs_train, padded_feat_train])\n",
    "\n",
    "    # evaluate the model\n",
    "#     padded_docs_test, _ = get_padded_docs(doc_test)\n",
    "#     predicted_list = model.predict(padded_docs_test)\n",
    "#     write_to_file(tweet_ids_test, emotions_test, doc_test, label_test, emotion + '_' + embedding_name+'_val_labels')\n",
    "#     predicted_list = [each[0] for each in predicted_list]\n",
    "#     write_to_file(tweet_ids_test, emotions_test,  doc_test, predicted_list, emotion+'_'+embedding_name+'_validation')\n",
    "\n",
    "    # dev set\n",
    "    tweet_ids, docs, emotions, labels = get_docs_contents(\n",
    "        os.path.join(dir_name, '..', 'data', 'en_dev', '2018-EI-reg-En-' + emotion + '-dev.txt'))\n",
    "    padded_docs_dev, t = get_padded_docs(docs)\n",
    "    predicted_list = model.predict(padded_docs_dev)\n",
    "    write_to_file(tweet_ids, emotions, docs, labels, emotion + '_' + embedding_name + '_dev_labels')\n",
    "    predicted_list = [each[0] for each in predicted_list]\n",
    "    write_to_file(tweet_ids, emotions, docs, predicted_list, emotion+'_'+embedding_name+'_dev')\n",
    "\n",
    "    print('Mean Squared Error of Validation Set: '+str(mean_squared_error(labels, predicted_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
