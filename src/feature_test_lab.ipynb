{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Explored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import math\n",
    "import pickle\n",
    "import gensim\n",
    "from textblob import Sentence\n",
    "\n",
    "delimiter = '\\t'\n",
    "dir_name = '/Users/nbarnaba/PycharmProjects/Keras_Samples/src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tweets_as_whole_text(df):\n",
    "    texts = df[1].values\n",
    "    whole_text = ''\n",
    "    for each in texts:\n",
    "        whole_text = whole_text + ' ' + each\n",
    "    return whole_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = os.path.join(dir_name, '..', 'data', 'en_train', 'EI-reg-en_anger_train.txt')\n",
    "df_anger = pd.read_csv(train_file_name, header=None, delimiter=delimiter)\n",
    "anger_texts = get_all_tweets_as_whole_text(df_anger)\n",
    "anger_hashtags = [each[0] for each in Counter(re.findall('#\\w+', anger_texts)).most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = os.path.join(dir_name, '..', 'data', 'en_train',  'EI-reg-en_fear_train.txt')\n",
    "df_fear = pd.read_csv(train_file_name, header=None, delimiter=delimiter)\n",
    "fear_texts = get_all_tweets_as_whole_text(df_fear)\n",
    "fear_hashtags = [each[0] for each in Counter(re.findall('#\\w+', fear_texts)).most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = os.path.join(dir_name, '..', 'data', 'en_train',  'EI-reg-en_sadness_train.txt')\n",
    "df_sadness = pd.read_csv(train_file_name, header=None, delimiter=delimiter)\n",
    "sadness_texts = get_all_tweets_as_whole_text(df_sadness)\n",
    "sadness_hashtags = [each[0] for each in Counter(re.findall('#\\w+', sadness_texts)).most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = os.path.join(dir_name, '..', 'data', 'en_train',  'EI-reg-en_joy_train.txt')\n",
    "df_joy = pd.read_csv(train_file_name, header=None, delimiter=delimiter)\n",
    "joy_texts = get_all_tweets_as_whole_text(df_joy)\n",
    "joy_hashtags = [each[0] for each in Counter(re.findall('#\\w+', joy_texts)).most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = anger_hashtags + joy_hashtags + sadness_hashtags + fear_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_hashtags = [each[0] for each in Counter(hashtags).most_common() if each[1]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anger : 483\n",
      "fear : 963\n",
      "joy : 529\n",
      "sadness : 582\n",
      "unique : 2557\n",
      "total : 3363\n"
     ]
    }
   ],
   "source": [
    "anger_hashtags = [each for each in anger_hashtags if each in unique_hashtags]\n",
    "fear_hashtags = [each for each in fear_hashtags if each in unique_hashtags]\n",
    "joy_hashtags = [each for each in joy_hashtags if each in unique_hashtags]\n",
    "sadness_hashtags = [each for each in sadness_hashtags if each in unique_hashtags]\n",
    "print('Anger : '+str(len(anger_hashtags)))\n",
    "print('fear : '+str(len(fear_hashtags)))\n",
    "print('joy : '+str(len(joy_hashtags)))\n",
    "print('sadness : '+str(len(sadness_hashtags)))\n",
    "print('unique : '+str(len(unique_hashtags)))\n",
    "print('total : '+str(len(hashtags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode Emo Hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1371909298\n",
      "          0                 1         2\n",
      "3908   fear         #westbank  1.951954\n",
      "3909   fear     #apprehension  1.951954\n",
      "3910   fear            #su4mh  1.951954\n",
      "3911   fear          aaaaaaah  1.951954\n",
      "3912   fear              #ied  1.951954\n",
      "3913   fear        #coldsweat  1.951954\n",
      "3914   fear             #isaf  1.951954\n",
      "3915   fear           cryotek  1.951954\n",
      "3916   fear          #rushing  1.951954\n",
      "3917   fear              #shy  1.951954\n",
      "3918   fear     #apprehensive  1.951954\n",
      "3919   fear            #mosul  1.951954\n",
      "3920   fear          #fearful  1.951954\n",
      "3921   fear           #ashdod  1.951954\n",
      "3922   fear    #socialanxiety  1.951954\n",
      "3923   fear        #backtrack  1.951954\n",
      "3924   fear         terrifies  1.951954\n",
      "3925   fear   #claustrophobia  1.951954\n",
      "3926   fear            qassam  1.951954\n",
      "3927   fear        #hezbollah  1.951954\n",
      "3928   fear          #hamas25  1.951954\n",
      "3929   fear              #dfw  1.951954\n",
      "3930   fear           #feared  1.951954\n",
      "3931   fear          #taliban  1.951954\n",
      "3932   fear       rationality  1.951954\n",
      "3933   fear  #israelunderfire  1.951954\n",
      "3934   fear     #ihatespiders  1.951954\n",
      "3935   fear         #pharmacy  1.951954\n",
      "3936   fear              #pij  1.951954\n",
      "3937   fear          #conquer  1.951954\n",
      "...     ...               ...       ...\n",
      "26997   joy           killers  0.008800\n",
      "26998   joy             psych  0.008800\n",
      "26999   joy           atlanta  0.008800\n",
      "27000   joy              laid  0.008800\n",
      "27001   joy            papers  0.008800\n",
      "27002   joy          showered  0.008800\n",
      "27003   joy              till  0.008547\n",
      "27004   joy             faces  0.006399\n",
      "27005   joy                24  0.006239\n",
      "27006   joy         finishing  0.006151\n",
      "27007   joy             ghost  0.005845\n",
      "27008   joy         therefore  0.005845\n",
      "27009   joy             films  0.005461\n",
      "27010   joy          survived  0.004961\n",
      "27011   joy            quotes  0.004961\n",
      "27012   joy               toy  0.004285\n",
      "27013   joy              safe  0.003569\n",
      "27014   joy        wallflower  0.003320\n",
      "27015   joy          monsters  0.003320\n",
      "27016   joy             frame  0.003320\n",
      "27017   joy          romantic  0.003320\n",
      "27018   joy           signing  0.003320\n",
      "27019   joy             pulls  0.003320\n",
      "27020   joy               100  0.003320\n",
      "27021   joy               dvd  0.003320\n",
      "27022   joy            sweets  0.003320\n",
      "27023   joy             apart  0.002558\n",
      "27024   joy            doctor  0.002035\n",
      "27025   joy              grad  0.001137\n",
      "27026   joy              1000  0.001137\n",
      "\n",
      "[15421 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "file_name = os.path.join(dir_name, '..','resources', 'NRC-Hashtag-Emotion-Lexicon-v0.2', 'NRC-Hashtag-Emotion-Lexicon-v0.2.txt')\n",
    "df_emo_hashtag = pd.read_csv(file_name, header=None, delimiter=delimiter)\n",
    "df_emo_hashtag = df_emo_hashtag[(df_emo_hashtag[0] == 'sadness') |\n",
    "                               (df_emo_hashtag[0] == 'anger') | \n",
    "                               (df_emo_hashtag[0] == 'fear') | \n",
    "                               (df_emo_hashtag[0] == 'joy')]\n",
    "print(df_emo_hashtag[(df_emo_hashtag[0] == 'anger') & (df_emo_hashtag[1] == 'pissed')][2].values[0])\n",
    "print(df_emo_hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['fear', '#westbank', 1.95195360556393],\n",
       "       ['fear', '#apprehension', 1.95195360556393],\n",
       "       ['fear', '#su4mh', 1.95195360556393],\n",
       "       ..., \n",
       "       ['joy', 'doctor', 0.0020351827030041197],\n",
       "       ['joy', 'grad', 0.00113671004597709],\n",
       "       ['joy', '1000', 0.00113671004597709]], dtype=object)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_hashtag = df_emo_hashtag.as_matrix()\n",
    "emo_hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy', 'grad', 0.00113671004597709], dtype=object)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_emo_hashtag(emotion, hashtag):\n",
    "    temp = None\n",
    "    for each in emo_hashtag: \n",
    "        if each[0] == emotion and (each[1] == hashtag):\n",
    "            if temp is None:\n",
    "                temp = each\n",
    "            else:\n",
    "                temp = np.concatenate((temp, each), axis=0)\n",
    "    return temp\n",
    "filter_emo_hashtag('joy', 'grad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(tweet_content):\n",
    "    return re.findall('#[a-zA-Z]+', tweet_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtag_emo_value(hashtag, emotion):\n",
    "    emo_value = filter_emo_hashtag(emotion, hashtag)\n",
    "    return emo_value[2] if emo_value is not None else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emot_value_from_hashtag(tweet_content, emotion):\n",
    "    hashtags = get_hashtags(tweet_content)\n",
    "    if len(hashtags) > 0:\n",
    "        _t_emo = []\n",
    "        for each_hashtag in hashtags:\n",
    "            _t_emo.append(get_hashtag_emo_value(each_hashtag, emotion))\n",
    "    else:\n",
    "        _t_emo = [0]\n",
    "    return np.mean(_t_emo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Unicode Emoticon Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "translation_table = dict.fromkeys(map(ord, string.punctuation), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['jeffreydonovan' 5.0 6 0]\n",
      " ['familar' 5.0 6 0]\n",
      " ['vppatel2011' 5.0 6 0]\n",
      " ..., \n",
      " ['clarianne' -4.999 0 5]\n",
      " ['scrambling' -4.999 0 8]\n",
      " ['ballsed' -4.999 0 6]]\n"
     ]
    }
   ],
   "source": [
    "file_name = os.path.join(dir_name, '..','resources', 'NRC-Emoticon-Lexicon-v1.0', 'Emoticon-unigrams.txt')\n",
    "df_emo_words = pd.read_csv(file_name, header=None, delimiter=delimiter)\n",
    "df_emo_words[0] = df_emo_words[0].str.lower().str.translate(translation_table)\n",
    "emo_words = df_emo_words.as_matrix() \n",
    "print(emo_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.999\n"
     ]
    }
   ],
   "source": [
    "def filter_emo_word(word):\n",
    "    temp = None\n",
    "    for each in emo_words: \n",
    "        if each[0] == word:\n",
    "            if temp is None:\n",
    "                temp = each\n",
    "            else:\n",
    "                temp = np.concatenate((temp, each), axis=0)\n",
    "    return temp\n",
    "print (filter_emo_word('ballsed')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ list(['how', 'the', 'fuk', 'who', 'the', 'heck', 'moved', 'my', 'fridge', 'should', 'i', 'knock', 'the', 'landlord', 'door', 'angry', 'mad']),\n",
       "       list(['so', 'my', 'indian', 'uber', 'driver', 'just', 'called', 'someone', 'the', 'n', 'word', 'if', 'i', 'wasnt', 'in', 'a', 'moving', 'vehicle', 'id', 'have', 'jumped', 'out', 'disgusted']),\n",
       "       list(['dpduk', 'i', 'asked', 'for', 'my', 'parcel', 'to', 'be', 'delivered', 'to', 'a', 'pick', 'up', 'store', 'not', 'my', 'address', 'fuming', 'poorcustomerservice']),\n",
       "       ...,\n",
       "       list(['id', 'love', '2', 'c', 'gyimah', 'in', 'action', 'but', 'his', 'coach', 'is', 'holding', 'a', 'grudge', 'against', 'him']),\n",
       "       list(['forgiving', 'means', 'operating', 'with', 'gods', 'spirit', 'amp', 'god', 'isnt', 'unforgiving', 'amp', 'doesnt', 'hold', 'a', 'grudge', 'so', 'who', 'am', 'i', 'exactly']),\n",
       "       list(['ive', 'got', 'a', 'lot', 'of', 'tokens', 'saved', 'up', 'and', 'i', 'wanna', 'spam', 'the', 'event', 'song', 'but', 'my', 'eyes', 'sting', 'im', 'so', 'tired'])], dtype=object)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tokenized_ndarray_from_df(df):\n",
    "    translation_table = dict.fromkeys(map(ord, string.punctuation), None)\n",
    "    return df[1].str.lower().str.translate(translation_table).str.split().values\n",
    "get_tokenized_ndarray_from_df(df_anger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emoticon_lexicon_value(df):\n",
    "    cleaned_tokenized = get_tokenized_ndarray_from_df(df)\n",
    "    emoticon_lexicon_value = []\n",
    "    for each_tweet in cleaned_tokenized:\n",
    "        emoti_word_count = 0\n",
    "        emoti_value = 0\n",
    "        for each_word in each_tweet:\n",
    "\n",
    "            tmp = filter_emo_word(each_word)\n",
    "            if tmp is not None:\n",
    "                emoti_value += tmp[1]\n",
    "                emoti_word_count += 1\n",
    "        emoti_value = (emoti_value)/(emoti_word_count+1)\n",
    "        emoticon_lexicon_value.append(emoti_value)\n",
    "    return emoticon_lexicon_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Vector (by averaging the constituent word vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_vectors():\n",
    "    global word_model\n",
    "    global dir_name\n",
    "#     embedding_file_loc = os.path.join(dir_name, '..', 'resources', 'GoogleNews-vectors-negative300.bin')\n",
    "    embedding_file_loc = os.path.join(dir_name, '..', 'resources', 'wiki.en', 'wiki.en.vec')\n",
    "    print(\"Loading the data file... Please wait...\")\n",
    "    word_model = gensim.models.KeyedVectors.load_word2vec_format(embedding_file_loc, binary=False)\n",
    "    print(\"Successfully loaded 3.6 G bin file!\")\n",
    "    return word_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase_vector_obj(value):\n",
    "    return PhraseVector(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhraseVector:\n",
    "    def __init__(self, phrase):\n",
    "        self.phrase = phrase\n",
    "        self.vector = self.phrase_to_vec(phrase)\n",
    "        self.pos_tag = self.get_words_in_phrase(phrase)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_vector_set_to_average(vector_set, ignore=[]):\n",
    "        if len(ignore) == 0:\n",
    "            return np.mean(vector_set, axis=0)\n",
    "        else:\n",
    "            return np.dot(np.transpose(vector_set), ignore) / sum(ignore)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_unique_token_tags(vector1, vector2):\n",
    "        tag_list = []\n",
    "        for each_tag in vector1.pos_tag + vector2.pos_tag:\n",
    "            if each_tag not in tag_list:\n",
    "                tag_list.append(each_tag)\n",
    "        return tag_list\n",
    "\n",
    "    def phrase_to_vec(self, phrase):\n",
    "        # _stop_words = stopwords.words(\"english\")\n",
    "        phrase = phrase.lower()\n",
    "        verified_words = [word for word in phrase.split()]\n",
    "        vector_set = []\n",
    "        for each_word in verified_words:\n",
    "            try:\n",
    "                word_vector = word_model[each_word]\n",
    "                vector_set.append(word_vector)\n",
    "            except:\n",
    "                pass\n",
    "        return self.convert_vector_set_to_average(vector_set)\n",
    "\n",
    "    def get_cosine_similarity(self, other_vector):\n",
    "        cosine_similarity = np.dot(self.vector, other_vector.vector) / (\n",
    "        np.linalg.norm(self.vector) * np.linalg.norm(other_vector.vector))\n",
    "        try:\n",
    "            if math.isnan(cosine_similarity):\n",
    "                cosine_similarity = 0\n",
    "        except:\n",
    "            cosine_similarity = 0\n",
    "        return cosine_similarity\n",
    "\n",
    "    def get_words_in_phrase(self, phrase):\n",
    "        if phrase.strip() == '':\n",
    "            return []\n",
    "        else:\n",
    "            tagged_input = nltk.pos_tag(phrase.split(), tagset='universal')\n",
    "            prev_item, prev_tag = tagged_input[0]\n",
    "            g_item_list = [prev_item]\n",
    "            cur_group_index = 0\n",
    "            space = ' '\n",
    "            revised_tag = []\n",
    "            for cur_item, cur_tag in tagged_input[1:]:\n",
    "                cur_item = cur_item.lower()\n",
    "                if prev_tag is cur_tag:\n",
    "                    g_item_list[cur_group_index] += space + cur_item\n",
    "                else:\n",
    "                    revised_tag.append((g_item_list[cur_group_index], prev_tag))\n",
    "                    prev_tag = cur_tag\n",
    "                    g_item_list.append(cur_item)\n",
    "                    cur_group_index += 1\n",
    "            revised_tag.append((g_item_list[cur_group_index], prev_tag))\n",
    "            return revised_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data file... Please wait...\n",
      "Successfully loaded 3.6 G bin file!\n",
      "<gensim.models.keyedvectors.KeyedVectors object at 0x12805d588>\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "word_model = load_word_vectors()\n",
    "print(word_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase_vectors(df, emotion):\n",
    "    tweet_vectors_obj = None\n",
    "    tweet_vectors = None\n",
    "    labels = None\n",
    "\n",
    "    tweet_vectors_obj = np.vectorize(get_phrase_vector_obj)(df[1].values)\n",
    "    tweet_vectors = np.array([[]])\n",
    "    for each_vector in tweet_vectors_obj:\n",
    "        curr_vector = each_vector.vector\n",
    "        if np.isnan(curr_vector).any():\n",
    "            curr_vector = np.zeros(shape=(1, 300))\n",
    "        else:\n",
    "            curr_vector = curr_vector.reshape(1, len(each_vector.vector))\n",
    "        if np.min(tweet_vectors.shape) == 0:\n",
    "            tweet_vectors = np.concatenate((tweet_vectors, curr_vector), axis=1)\n",
    "        else:\n",
    "            tweet_vectors = np.concatenate((tweet_vectors, curr_vector), axis=0)\n",
    "    labels = df[3].values\n",
    "\n",
    "    return tweet_vectors, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarity and Subjectivity using Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polarity_and_subjectivity(df, emotion):\n",
    "    polarity_list = []\n",
    "    subjectivity_list = []\n",
    "\n",
    "    polarity_list = np.array(list(map(lambda x: Sentence(x).polarity, df[1].values)))\n",
    "    subjectivity_list = np.array(list(map(lambda x: Sentence(x).subjectivity, df[1].values)))\n",
    "\n",
    "    polarity_list = polarity_list.reshape(len(polarity_list),1)\n",
    "    subjectivity_list = subjectivity_list.reshape(len(subjectivity_list),1)\n",
    "    return polarity_list, subjectivity_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Features Concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(df, emotion):\n",
    "    print('inside get_features...')\n",
    "    polarity_list, subjectivity_list = get_polarity_and_subjectivity(df, emotion)\n",
    "    tweet_vectors, labels = get_phrase_vectors(df, emotion)\n",
    "    emoticon_lexicon_value = get_emoticon_lexicon_value(df)\n",
    "    emoticon_lexicon_value = np.array(emoticon_lexicon_value).reshape(len(emoticon_lexicon_value),1)\n",
    "    hashtag_feature_nrc_hashtag_emoticon = np.array([get_emot_value_from_hashtag(each, emotion) \n",
    "                                                       for each in df.as_matrix()[:,1]]).reshape(len(df), 1)\n",
    "    return np.concatenate((polarity_list, \n",
    "                           subjectivity_list, \n",
    "                           tweet_vectors, \n",
    "                           emoticon_lexicon_value,\n",
    "                           hashtag_feature_nrc_hashtag_emoticon), axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.constraints import min_max_norm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "\n",
    "max_length = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_parameters(features):\n",
    "    vocab_size, embedded_vector_length = features.shape\n",
    "    max_length = vocab_size\n",
    "    embedding_matrix = features\n",
    "    return vocab_size, embedded_vector_length, max_length, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_model(vocab_size, embedded_vector_length, embedding_matrix, max_length, \n",
    "                  optimizer='adam',\n",
    "                  loss='mean_squared_logarithmic_error'):\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size, embedded_vector_length, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "    model.add(e)\n",
    "    lstm = Bidirectional(LSTM(300, activation='relu',\n",
    "                              kernel_initializer='random_uniform',\n",
    "                              bias_initializer='zeros',\n",
    "                              kernel_constraint=min_max_norm(min_value=-1.0, max_value=1.0, rate=0.5, axis=0),\n",
    "                              bias_constraint=min_max_norm(min_value=-1.0, max_value=1.0, rate=0.5, axis=0)))\n",
    "    model.add(lstm)\n",
    "    model.add(Dense(300, activation='relu'))\n",
    "#     # compile the model\n",
    "#     model.compile(optimizer=optimizer, loss=loss)\n",
    "#     # summarize the model\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense_model(vocab_size, embedded_vector_length, embedding_matrix, max_length, \n",
    "                  optimizer='adam',\n",
    "                  loss='mean_squared_logarithmic_error'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(300, \n",
    "                    activation='relu', \n",
    "                    input_shape=(embedded_vector_length,)))\n",
    "#     # compile the model\n",
    "#     model.compile(optimizer=optimizer, loss=loss)\n",
    "#     # summarize the model\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnn_model(embedding_models,\n",
    "                  optimizer='adam',\n",
    "                  loss='mean_squared_logarithmic_error',\n",
    "                  output_activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Merge(embedding_models, mode='concat', concat_axis=1))\n",
    "    model.add(Dense(1, activation=output_activation,\n",
    "                    kernel_initializer='random_uniform',\n",
    "                    bias_initializer='zeros',\n",
    "                    kernel_constraint=min_max_norm(min_value=-1.0, max_value=1.0, rate=.5, axis=0),\n",
    "                    bias_constraint=min_max_norm(min_value=-1.0, max_value=1.0, rate=0.5, axis=0)))\n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "    return model\n",
    "    # fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_index(embedding_file_name):\n",
    "    # load the whole embedding into memory\n",
    "    embeddings_index = dict()\n",
    "    # f = open('../resources/glove.6B/glove.6B.'+str(embedded_vector_length)+'d.txt')\n",
    "    with open(embedding_file_name) as f:\n",
    "        l_no = 0\n",
    "        for line in f:\n",
    "            if l_no == 0:\n",
    "                l_no += 1\n",
    "                continue\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            try:\n",
    "                coefs = asarray(values[1:], dtype='float32')\n",
    "            except ValueError as e:\n",
    "                coefs = asarray(values[-embedded_vector_length:], dtype='float32')\n",
    "            if word not in embeddings_index.keys():\n",
    "                embeddings_index[word] = coefs\n",
    "    return  embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(tweet_ids, assgn_emotions, tweet_contents, predicted_scores, file_name):\n",
    "    dir_name = '/Users/nbarnaba/PycharmProjects/Keras_Samples/src'\n",
    "    if not os.path.exists(os.path.join(dir_name, '..', 'output')):\n",
    "        os.makedirs(os.path.join(dir_name, '..', 'output'))\n",
    "    with open(os.path.join(dir_name, '..', 'output', file_name), 'w') as f:\n",
    "        file_writer = csv.writer(f, delimiter='\\t')\n",
    "        for each_tweet_id, each_tweet_content, each_emotion, each_score in \\\n",
    "                zip(tweet_ids, tweet_contents, assgn_emotions, predicted_scores):\n",
    "            file_writer.writerow([each_tweet_id, each_tweet_content, each_emotion, each_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(df, embeddings_index=None):\n",
    "    cleaned_tokenized = get_tokenized_ndarray_from_df(df)\n",
    "    unique_tokens = set()\n",
    "    for each_tweet in cleaned_tokenized:\n",
    "        filtered_set = []\n",
    "        if embeddings_index is None:\n",
    "            filtered_set = each_tweet\n",
    "        else:\n",
    "            for each_word in each_tweet:\n",
    "                if each_word in embeddings_index.keys():\n",
    "                    filtered_set.append(each_word)\n",
    "        unique_tokens = unique_tokens.union(set(filtered_set))\n",
    "    unique_tokens = list(unique_tokens)\n",
    "    vocab = {each_word : each_index+1 for each_index, each_word in enumerate(unique_tokens)}\n",
    "    vocab['<unk>' ] = 0\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(vocab, embeddings_index, embedded_vector_length=300):\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = zeros((vocab_size, embedded_vector_length))\n",
    "    for word in vocab.keys():\n",
    "        if word in embeddings_index.keys():\n",
    "            embedding_matrix[vocab[word]] = embeddings_index[word]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encode_docs(df, vocab):\n",
    "    cleaned_tokenized = get_tokenized_ndarray_from_df(df)\n",
    "    encoded_docs = []\n",
    "    max_length = -1\n",
    "    for each_tweet in cleaned_tokenized:\n",
    "        encoded_tokenized_tweet = []\n",
    "        for each_word in each_tweet:\n",
    "            if each_word in vocab.keys():\n",
    "                encoded_tokenized_tweet.append(vocab[each_word])\n",
    "            else:\n",
    "                encoded_tokenized_tweet.append(0)\n",
    "        current_length = len(encoded_tokenized_tweet)\n",
    "        if current_length > max_length:\n",
    "            max_length = current_length\n",
    "        encoded_docs.append(encoded_tokenized_tweet)\n",
    "    return encoded_docs, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_docs(df, vocab, encoded_size=None):\n",
    "    encoded_docs, max_length = get_encode_docs(df, vocab)\n",
    "    padded_docs = np.array([[]])\n",
    "    if encoded_size is not None:\n",
    "        max_length = encoded_size\n",
    "    for each_encoded_doc in encoded_docs:\n",
    "        each_encoded_size = len(each_encoded_doc)\n",
    "        each_encoded_doc = np.array(each_encoded_doc)\n",
    "        if max_length >= each_encoded_size: \n",
    "            each_encoded_doc = np.array([np.pad(each_encoded_doc, (0, max_length-each_encoded_size), 'constant')])\n",
    "        else:\n",
    "            each_encoded_doc = np.array([each_encoded_doc[:max_length]])\n",
    "        if padded_docs.size == 0:\n",
    "            padded_docs = np.concatenate((padded_docs, each_encoded_doc), axis=1)\n",
    "        else:\n",
    "            padded_docs = np.concatenate((padded_docs, each_encoded_doc), axis=0)\n",
    "    return padded_docs, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2518843 word vectors.\n",
      "Built vocabulary...\n",
      "Padded docs for training is created...\n",
      "Loaded embedding matrix...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 33, 300)           1811100   \n",
      "_________________________________________________________________\n",
      "bidirectional_15 (Bidirectio (None, 600)               1442400   \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 300)               180300    \n",
      "=================================================================\n",
      "Total params: 3,433,800\n",
      "Trainable params: 1,622,700\n",
      "Non-trainable params: 1,811,100\n",
      "_________________________________________________________________\n",
      "None\n",
      "inside get_features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "dir_name = '/Users/nbarnaba/PycharmProjects/Keras_Samples/src'\n",
    "char_level = False\n",
    "docs = []\n",
    "labels = []\n",
    "tweet_ids = []\n",
    "emotions = []\n",
    "\n",
    "embedded_vector_length = 300\n",
    "emotion_names = ['sadness', 'joy', 'anger', 'fear']\n",
    "embedding_name = 'wiki.en'\n",
    "embeddings_index = get_embedding_index(os.path.join(dir_name,\n",
    "                                                    '..',\n",
    "                                                    'resources',\n",
    "                                                    embedding_name,\n",
    "                                                    'wiki.en.vec'))\n",
    "                                                    # 'glove.6B.' + str(embedded_vector_length) + 'd.txt'))\n",
    "\n",
    "for emot_id, emotion in enumerate(emotion_names):\n",
    "    training_file_name = os.path.join(dir_name, '..','data','en_train','EI-reg-en_'+emotion+'_train.txt')\n",
    "    \n",
    "    df = pd.read_csv(training_file_name, header=None, delimiter=delimiter)\n",
    "    tweet_ids_train, docs_train, emotions_train, label_train = [df[each].values for each in range(4)]\n",
    "\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    vocab = build_vocab(df)\n",
    "    print('Built vocabulary...')\n",
    "    padded_docs_train, em_max_length = get_padded_docs(df, vocab)\n",
    "    print('Padded docs for training is created...')\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = build_embedding_matrix(vocab, embeddings_index)\n",
    "    print('Loaded embedding matrix...')\n",
    "    # define model with word embeddings\n",
    "    _, embedded_vector_length = embedding_matrix.shape\n",
    "    word_model = get_embedding_model(vocab_size, embedded_vector_length, embedding_matrix, em_max_length)\n",
    "    # Added features\n",
    "    current_feature = get_features(df, emotion)\n",
    "    print('extra features loaded...')\n",
    "    print('loaded word model...')\n",
    "    vocab_size, embedded_vector_length, max_length, embedding_matrix = get_embedding_parameters(current_feature)\n",
    "\n",
    "    phrase_vector_model = get_dense_model(vocab_size, embedded_vector_length, embedding_matrix, max_length)\n",
    "    print('loaded phrase model...')\n",
    "    model=get_rnn_model([word_model, phrase_vector_model])\n",
    "    \n",
    "    print(padded_docs_train.shape)\n",
    "    print(current_feature.shape)\n",
    "    \n",
    "    model.fit([padded_docs_train, current_feature], label_train, verbose=1)\n",
    "    \n",
    "    print('model has been fit...')\n",
    "    # dev set\n",
    "    dev_file_name=os.path.join(dir_name, '..', 'data', 'en_dev', '2018-EI-reg-En-' + emotion + '-dev.txt')\n",
    "    df = pd.read_csv(dev_file_name, header=None, delimiter=delimiter)\n",
    "    tweet_ids, docs, emotions, labels = [df[each].values for each in range(4)]\n",
    "    padded_docs_dev, _ = get_padded_docs(df, vocab, encoded_size = em_max_length)\n",
    "    dev_features = get_features(df, emotion)\n",
    "    \n",
    "    predicted_list = model.predict([padded_docs_dev, dev_features])\n",
    "    write_to_file(tweet_ids, emotions, docs, labels, emotion + '_' + embedding_name + '_dev_labels')\n",
    "    predicted_list = [each[0] for each in predicted_list]\n",
    "    write_to_file(tweet_ids, emotions, docs, predicted_list, emotion+'_'+embedding_name+'_dev')\n",
    "\n",
    "    print('Mean Squared Error of Validation Set: '+str(mean_squared_error(labels, predicted_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5834, 300)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
