{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Explored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import math\n",
    "import pickle\n",
    "import gensim\n",
    "from textblob import Sentence\n",
    "\n",
    "delimiter = '\\t'\n",
    "dir_name = '/home/dennis/PycharmProjects/Keras_Samples/src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_all_tweets_as_whole_text(df):\n",
    "    texts = df[1].values\n",
    "    whole_text = ''\n",
    "    for each in texts:\n",
    "        whole_text = whole_text + ' ' + each\n",
    "    return whole_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file_name = os.path.join(dir_name, '..', 'data', 'en_train', 'EI-reg-en_anger_train.txt')\n",
    "df_anger = pd.read_csv(train_file_name, header=None, delimiter=delimiter)\n",
    "anger_texts = get_all_tweets_as_whole_text(df_anger)\n",
    "anger_hashtags = [each[0] for each in Counter(re.findall('#\\w+', whole_text)).most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file_name = os.path.join(dir_name, '..', 'data', 'en_train',  'EI-reg-en_fear_train.txt')\n",
    "df_fear = pd.read_csv(train_file_name, header=None, delimiter=delimiter)\n",
    "fear_texts = get_all_tweets_as_whole_text(df_fear)\n",
    "fear_hashtags = [each[0] for each in Counter(re.findall('#\\w+', whole_text)).most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file_name = os.path.join(dir_name, '..', 'data', 'en_train',  'EI-reg-en_sadness_train.txt')\n",
    "df_sadness = pd.read_csv(train_file_name, header=None, delimiter=delimiter)\n",
    "sadness_texts = get_all_tweets_as_whole_text(df_sadness)\n",
    "sadness_hashtags = [each[0] for each in Counter(re.findall('#\\w+', whole_text)).most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file_name = os.path.join(dir_name, '..', 'data', 'en_train',  'EI-reg-en_joy_train.txt')\n",
    "df_joy = pd.read_csv(train_file_name, header=None, delimiter=delimiter)\n",
    "joy_texts = get_all_tweets_as_whole_text(df_joy)\n",
    "joy_hashtags = [each[0] for each in Counter(re.findall('#\\w+', whole_text)).most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtags = anger_hashtags + joy_hashtags + sadness_hashtags + fear_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_hashtags = [each[0] for each in Counter(hashtags).most_common() if each[1]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anger : 483\n",
      "fear : 963\n",
      "joy : 529\n",
      "sadness : 582\n",
      "unique : 2557\n",
      "total : 3363\n"
     ]
    }
   ],
   "source": [
    "anger_hashtags = [each for each in anger_hashtags if each in unique_hashtags]\n",
    "fear_hashtags = [each for each in fear_hashtags if each in unique_hashtags]\n",
    "joy_hashtags = [each for each in joy_hashtags if each in unique_hashtags]\n",
    "sadness_hashtags = [each for each in sadness_hashtags if each in unique_hashtags]\n",
    "print('Anger : '+str(len(anger_hashtags)))\n",
    "print('fear : '+str(len(fear_hashtags)))\n",
    "print('joy : '+str(len(joy_hashtags)))\n",
    "print('sadness : '+str(len(sadness_hashtags)))\n",
    "print('unique : '+str(len(unique_hashtags)))\n",
    "print('total : '+str(len(hashtags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode Emo Hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1371909298\n",
      "          0                 1         2\n",
      "3908   fear         #westbank  1.951954\n",
      "3909   fear     #apprehension  1.951954\n",
      "3910   fear            #su4mh  1.951954\n",
      "3911   fear          aaaaaaah  1.951954\n",
      "3912   fear              #ied  1.951954\n",
      "3913   fear        #coldsweat  1.951954\n",
      "3914   fear             #isaf  1.951954\n",
      "3915   fear           cryotek  1.951954\n",
      "3916   fear          #rushing  1.951954\n",
      "3917   fear              #shy  1.951954\n",
      "3918   fear     #apprehensive  1.951954\n",
      "3919   fear            #mosul  1.951954\n",
      "3920   fear          #fearful  1.951954\n",
      "3921   fear           #ashdod  1.951954\n",
      "3922   fear    #socialanxiety  1.951954\n",
      "3923   fear        #backtrack  1.951954\n",
      "3924   fear         terrifies  1.951954\n",
      "3925   fear   #claustrophobia  1.951954\n",
      "3926   fear            qassam  1.951954\n",
      "3927   fear        #hezbollah  1.951954\n",
      "3928   fear          #hamas25  1.951954\n",
      "3929   fear              #dfw  1.951954\n",
      "3930   fear           #feared  1.951954\n",
      "3931   fear          #taliban  1.951954\n",
      "3932   fear       rationality  1.951954\n",
      "3933   fear  #israelunderfire  1.951954\n",
      "3934   fear     #ihatespiders  1.951954\n",
      "3935   fear         #pharmacy  1.951954\n",
      "3936   fear              #pij  1.951954\n",
      "3937   fear          #conquer  1.951954\n",
      "...     ...               ...       ...\n",
      "26997   joy           killers  0.008800\n",
      "26998   joy             psych  0.008800\n",
      "26999   joy           atlanta  0.008800\n",
      "27000   joy              laid  0.008800\n",
      "27001   joy            papers  0.008800\n",
      "27002   joy          showered  0.008800\n",
      "27003   joy              till  0.008547\n",
      "27004   joy             faces  0.006399\n",
      "27005   joy                24  0.006239\n",
      "27006   joy         finishing  0.006151\n",
      "27007   joy             ghost  0.005845\n",
      "27008   joy         therefore  0.005845\n",
      "27009   joy             films  0.005461\n",
      "27010   joy          survived  0.004961\n",
      "27011   joy            quotes  0.004961\n",
      "27012   joy               toy  0.004285\n",
      "27013   joy              safe  0.003569\n",
      "27014   joy        wallflower  0.003320\n",
      "27015   joy          monsters  0.003320\n",
      "27016   joy             frame  0.003320\n",
      "27017   joy          romantic  0.003320\n",
      "27018   joy           signing  0.003320\n",
      "27019   joy             pulls  0.003320\n",
      "27020   joy               100  0.003320\n",
      "27021   joy               dvd  0.003320\n",
      "27022   joy            sweets  0.003320\n",
      "27023   joy             apart  0.002558\n",
      "27024   joy            doctor  0.002035\n",
      "27025   joy              grad  0.001137\n",
      "27026   joy              1000  0.001137\n",
      "\n",
      "[15421 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "file_name = os.path.join(dir_name, '..','resources', 'NRC-Hashtag-Emotion-Lexicon-v0.2', 'NRC-Hashtag-Emotion-Lexicon-v0.2.txt')\n",
    "df_emo_hashtag = pd.read_csv(file_name, header=None, delimiter=delimiter)\n",
    "df_emo_hashtag = df_emo_hashtag[(df_emo_hashtag[0] == 'sadness') |\n",
    "                               (df_emo_hashtag[0] == 'anger') | \n",
    "                               (df_emo_hashtag[0] == 'fear') | \n",
    "                               (df_emo_hashtag[0] == 'joy')]\n",
    "print(df_emo_hashtag[(df_emo_hashtag[0] == 'anger') & (df_emo_hashtag[1] == 'pissed')][2].values[0])\n",
    "print(df_emo_hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['fear', '#westbank', 1.95195360556393],\n",
       "       ['fear', '#apprehension', 1.95195360556393],\n",
       "       ['fear', '#su4mh', 1.95195360556393],\n",
       "       ..., \n",
       "       ['joy', 'doctor', 0.0020351827030041197],\n",
       "       ['joy', 'grad', 0.00113671004597709],\n",
       "       ['joy', '1000', 0.00113671004597709]], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_hashtag = df_emo_hashtag.as_matrix()\n",
    "emo_hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy', 'grad', 0.00113671004597709], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_emo_hashtag(emotion, hashtag):\n",
    "    temp = None\n",
    "    for each in emo_hashtag: \n",
    "        if each[0] == emotion and (each[1] == hashtag):\n",
    "            if temp is None:\n",
    "                temp = each\n",
    "            else:\n",
    "                temp = np.concatenate((temp, each), axis=0)\n",
    "    return temp\n",
    "filter_emo_hashtag('joy', 'grad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hashtags(tweet_content):\n",
    "    return re.findall('#[a-zA-Z]+', tweet_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hashtag_emo_value(hashtag, emotion):\n",
    "    emo_value = filter_emo_hashtag(emotion, hashtag)\n",
    "    return emo_value[2] if emo_value is not None else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_emot_value_from_hashtag(tweet_content, emotion):\n",
    "    hashtags = get_hashtags(tweet_content)\n",
    "    if len(hashtags) > 0:\n",
    "        _t_emo = []\n",
    "        for each_hashtag in hashtags:\n",
    "            _t_emo.append(get_hashtag_emo_value(each_hashtag, emotion))\n",
    "    else:\n",
    "        _t_emo = [0]\n",
    "    return np.mean(_t_emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anger_hashtag_feature_nrc_hashtag_emoticon = np.array([get_emot_value_from_hashtag(each, 'anger') \n",
    "                                                       for each in df_anger.as_matrix()[:,1]]).reshape(len(df_anger), 1)\n",
    "joy_hashtag_feature_nrc_hashtag_emoticon = np.array([get_emot_value_from_hashtag(each, 'joy') \n",
    "                                                     for each in df_joy.as_matrix()[:,1]]).reshape(len(df_joy), 1)\n",
    "fear_hashtag_feature_nrc_hashtag_emoticon = np.array([get_emot_value_from_hashtag(each, 'fear')\n",
    "                                                      for each in df_fear.as_matrix()[:,1]]).reshape(len(df_fear), 1)\n",
    "sadness_hashtag_feature_nrc_hashtag_emoticon = np.array([get_emot_value_from_hashtag(each, 'sadness') \n",
    "                                                         for each in df_sadness.as_matrix()[:,1]]).reshape(len(df_sadness), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1701, 1)\n",
      "(1616, 1)\n",
      "(2252, 1)\n",
      "(1533, 1)\n"
     ]
    }
   ],
   "source": [
    "print(anger_hashtag_feature_nrc_hashtag_emoticon.shape)\n",
    "print(joy_hashtag_feature_nrc_hashtag_emoticon.shape)\n",
    "print(fear_hashtag_feature_nrc_hashtag_emoticon.shape)\n",
    "print(sadness_hashtag_feature_nrc_hashtag_emoticon.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Unicode Emoticon Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "translation_table = dict.fromkeys(map(ord, string.punctuation), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['jeffreydonovan' 5.0 6 0]\n",
      " ['familar' 5.0 6 0]\n",
      " ['vppatel2011' 5.0 6 0]\n",
      " ..., \n",
      " ['clarianne' -4.999 0 5]\n",
      " ['scrambling' -4.999 0 8]\n",
      " ['ballsed' -4.999 0 6]]\n"
     ]
    }
   ],
   "source": [
    "file_name = os.path.join(dir_name, '..','resources', 'NRC-Emoticon-Lexicon-v1.0', 'Emoticon-unigrams.txt')\n",
    "df_emo_words = pd.read_csv(file_name, header=None, delimiter=delimiter)\n",
    "df_emo_words[0] = df_emo_words[0].str.lower().str.translate(translation_table)\n",
    "emo_words = df_emo_words.as_matrix() \n",
    "print(emo_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.999\n"
     ]
    }
   ],
   "source": [
    "def filter_emo_word(word):\n",
    "    temp = None\n",
    "    for each in emo_words: \n",
    "        if each[0] == word:\n",
    "            if temp is None:\n",
    "                temp = each\n",
    "            else:\n",
    "                temp = np.concatenate((temp, each), axis=0)\n",
    "    return temp\n",
    "print (filter_emo_word('ballsed')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_emoticon_lexicon_value(df):\n",
    "    translation_table = dict.fromkeys(map(ord, string.punctuation), None)\n",
    "    cleaned_tokenized = df[1].str.lower().str.translate(translation_table).str.split().values\n",
    "    emoticon_lexicon_value = []\n",
    "    for each_tweet in cleaned_tokenized:\n",
    "        emoti_word_count = 0\n",
    "        emoti_value = 0\n",
    "        for each_word in each_tweet:\n",
    "\n",
    "            tmp = filter_emo_word(each_word)\n",
    "            if tmp is not None:\n",
    "                emoti_value += tmp[1]\n",
    "                emoti_word_count += 1\n",
    "        emoti_value = (emoti_value)/(emoti_word_count+1)\n",
    "        emoticon_lexicon_value.append(emoti_value)\n",
    "    return emoticon_lexicon_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anger_emoticon_lexicon_value = get_emoticon_lexicon_value(df_anger)\n",
    "joy_emoticon_lexicon_value = get_emoticon_lexicon_value(df_joy)\n",
    "sadness_emoticon_lexicon_value = get_emoticon_lexicon_value(df_sadness)\n",
    "fear_emoticon_lexicon_value = get_emoticon_lexicon_value(df_fear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joy_emoticon_lexicon_value = np.array(joy_emoticon_lexicon_value).reshape(len(joy_emoticon_lexicon_value),1)\n",
    "anger_emoticon_lexicon_value = np.array(anger_emoticon_lexicon_value).reshape(len(anger_emoticon_lexicon_value),1)\n",
    "fear_emoticon_lexicon_value = np.array(fear_emoticon_lexicon_value).reshape(len(fear_emoticon_lexicon_value),1)\n",
    "sadness_emoticon_lexicon_value = np.array(sadness_emoticon_lexicon_value).reshape(len(sadness_emoticon_lexicon_value),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Vector (by averaging the constituent word vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_word_vectors():\n",
    "    global word_model\n",
    "    global dir_name\n",
    "    if not os.path.exists(os.path.join(dir_name, '..', 'output')):\n",
    "        os.makedirs(os.path.join(dir_name, '..', 'output'))\n",
    "    model_filename = 'GoogleWord2Vec'\n",
    "    model_filename = os.path.join(dir_name, '..', 'resources', model_filename)\n",
    "    if not os.path.exists(model_filename):\n",
    "        embedding_file_loc = os.path.join(dir_name, '..', 'resources', 'GoogleNews-vectors-negative300.bin')\n",
    "        print(\"Loading the data file... Please wait...\")\n",
    "        word_model = gensim.models.KeyedVectors.load_word2vec_format(embedding_file_loc, binary=True)\n",
    "        print(\"Successfully loaded 3.6 G bin file!\")\n",
    "        pickle.dump(word_model, open(model_filename, 'wb'))\n",
    "    else:\n",
    "        word_model = pickle.load(open(model_filename, 'rb'))\n",
    "        print('Successfully Loaded the model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_phrase_vector_obj(value):\n",
    "    return PhraseVector(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PhraseVector:\n",
    "    def __init__(self, phrase):\n",
    "        self.phrase = phrase\n",
    "        self.vector = self.phrase_to_vec(phrase)\n",
    "        self.pos_tag = self.get_words_in_phrase(phrase)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_vector_set_to_average(vector_set, ignore=[]):\n",
    "        if len(ignore) == 0:\n",
    "            return np.mean(vector_set, axis=0)\n",
    "        else:\n",
    "            return np.dot(np.transpose(vector_set), ignore) / sum(ignore)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_unique_token_tags(vector1, vector2):\n",
    "        tag_list = []\n",
    "        for each_tag in vector1.pos_tag + vector2.pos_tag:\n",
    "            if each_tag not in tag_list:\n",
    "                tag_list.append(each_tag)\n",
    "        return tag_list\n",
    "\n",
    "    def phrase_to_vec(self, phrase):\n",
    "        # _stop_words = stopwords.words(\"english\")\n",
    "        phrase = phrase.lower()\n",
    "        verified_words = [word for word in phrase.split()]\n",
    "        vector_set = []\n",
    "        for each_word in verified_words:\n",
    "            try:\n",
    "                word_vector = word_model[each_word]\n",
    "                vector_set.append(word_vector)\n",
    "            except:\n",
    "                pass\n",
    "        return self.convert_vector_set_to_average(vector_set)\n",
    "\n",
    "    def get_cosine_similarity(self, other_vector):\n",
    "        cosine_similarity = np.dot(self.vector, other_vector.vector) / (\n",
    "        np.linalg.norm(self.vector) * np.linalg.norm(other_vector.vector))\n",
    "        try:\n",
    "            if math.isnan(cosine_similarity):\n",
    "                cosine_similarity = 0\n",
    "        except:\n",
    "            cosine_similarity = 0\n",
    "        return cosine_similarity\n",
    "\n",
    "    def get_words_in_phrase(self, phrase):\n",
    "        if phrase.strip() == '':\n",
    "            return []\n",
    "        else:\n",
    "            tagged_input = nltk.pos_tag(phrase.split(), tagset='universal')\n",
    "            prev_item, prev_tag = tagged_input[0]\n",
    "            g_item_list = [prev_item]\n",
    "            cur_group_index = 0\n",
    "            space = ' '\n",
    "            revised_tag = []\n",
    "            for cur_item, cur_tag in tagged_input[1:]:\n",
    "                cur_item = cur_item.lower()\n",
    "                if prev_tag is cur_tag:\n",
    "                    g_item_list[cur_group_index] += space + cur_item\n",
    "                else:\n",
    "                    revised_tag.append((g_item_list[cur_group_index], prev_tag))\n",
    "                    prev_tag = cur_tag\n",
    "                    g_item_list.append(cur_item)\n",
    "                    cur_group_index += 1\n",
    "            revised_tag.append((g_item_list[cur_group_index], prev_tag))\n",
    "            return revised_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Loaded the model\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "load_word_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_phrase_vectors(df, emotion):\n",
    "    tweet_vectors_obj = None\n",
    "    tweet_vectors = None\n",
    "    labels = None\n",
    "    filename = os.path.join(dir_name, '..', 'resources', 'raw_phrase_vectors_obj_'+emotion)\n",
    "    if not os.path.exists(filename):\n",
    "        tweet_vectors_obj = np.vectorize(get_phrase_vector_obj)(df[1].values)\n",
    "        tweet_vectors = np.array([[]])\n",
    "        for each_vector in tweet_vectors_obj:\n",
    "            curr_vector = each_vector.vector\n",
    "            if np.isnan(curr_vector).any():\n",
    "                curr_vector = np.zeros(shape=(1, 300))\n",
    "            else:\n",
    "                curr_vector = curr_vector.reshape(1, len(each_vector.vector))\n",
    "            if np.min(tweet_vectors.shape) == 0:\n",
    "                tweet_vectors = np.concatenate((tweet_vectors, curr_vector), axis=1)\n",
    "            else:\n",
    "                tweet_vectors = np.concatenate((tweet_vectors, curr_vector), axis=0)\n",
    "        labels = df[3].values\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(tweet_vectors_obj, f)\n",
    "            pickle.dump(tweet_vectors, f)\n",
    "            pickle.dump(labels, f)\n",
    "    else:\n",
    "        with open(filename, 'rb') as f:\n",
    "            tweet_vectors_obj = pickle.load(f)\n",
    "            tweet_vectors = pickle.load(f)\n",
    "            labels = pickle.load(f)\n",
    "    return tweet_vectors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dennis/.local/lib/python3.5/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/dennis/.local/lib/python3.5/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "tweet_vectors_anger, labels_anger = get_phrase_vectors(df_anger, emotion='anger')\n",
    "tweet_vectors_joy, labels_joy = get_phrase_vectors(df_joy, emotion='joy')\n",
    "tweet_vectors_sadness, labels_sadness = get_phrase_vectors(df_sadness, emotion='sadness')\n",
    "tweet_vectors_fear, labels_fear = get_phrase_vectors(df_fear, emotion='fear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_polarity_and_subjectivity(df, emotion):\n",
    "    polarity_list = []\n",
    "    subjectivity_list = []\n",
    "    filename = os.path.join(dir_name, '..', 'resources', 'polarity_and_subjectivity_'+emotion)\n",
    "    if not os.path.exists(filename):\n",
    "        polarity_list = np.array(list(map(lambda x: Sentence(x).polarity, df[1].values)))\n",
    "        subjectivity_list = np.array(list(map(lambda x: Sentence(x).subjectivity, df[1].values)))\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(polarity_list, f)\n",
    "            pickle.dump(subjectivity_list, f)\n",
    "    else:\n",
    "        with open(filename, 'rb') as f:\n",
    "            polarity_list = pickle.load(f)\n",
    "            subjectivity_list = pickle.load(f)\n",
    "    polarity_list = polarity_list.reshape(len(polarity_list),1)\n",
    "    subjectivity_list = subjectivity_list.reshape(len(subjectivity_list),1)\n",
    "    return polarity_list, subjectivity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_list_anger, subjectivity_list_anger = get_polarity_and_subjectivity(df_anger, emotion='anger')\n",
    "polarity_list_fear, subjectivity_list_fear = get_polarity_and_subjectivity(df_fear, emotion='fear')\n",
    "polarity_list_sadness, subjectivity_list_sadness = get_polarity_and_subjectivity(df_sadness, emotion='sadness')\n",
    "polarity_list_joy, subjectivity_list_joy = get_polarity_and_subjectivity(df_joy, emotion='joy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3       ],\n",
       "       [ 0.9       ],\n",
       "       [ 0.9       ],\n",
       "       ..., \n",
       "       [ 0.6       ],\n",
       "       [ 0.66666667],\n",
       "       [ 0.53333333]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
