{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Explored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><u>Note about the dataframes used when reading the dataset:</u></h3><br>\n",
    "<b>`df[1]`</b> is used for selecting the tweet column in the dataframe throughout this script because there are no headers in the file. And if it is added, then these commands have to be replaced with the column names that are specified. Same goes for positional indexing in other columns during feature extraction - they have to be replaced with the corresponding column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import math\n",
    "import pickle\n",
    "import gensim\n",
    "from textblob import Sentence\n",
    "import csv\n",
    "import string\n",
    "from nltk.util import ngrams\n",
    "\n",
    "delimiter = '\\t'\n",
    "dir_name = '/Users/nbarnaba/PycharmProjects/Keras_Samples/src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tweets_as_whole_text(df):\n",
    "    texts = df[1].values\n",
    "    whole_text = ''\n",
    "    for each in texts:\n",
    "        whole_text = whole_text + ' ' + each\n",
    "    return whole_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = os.path.join(dir_name, '..', 'data', 'en_train', 'EI-reg-en_anger_train.txt')\n",
    "df_anger = pd.read_csv(train_file_name, header=None, delimiter=delimiter)\n",
    "anger_texts = get_all_tweets_as_whole_text(df_anger)\n",
    "anger_hashtags = [each[0] for each in Counter(re.findall('#\\w+', anger_texts)).most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = os.path.join(dir_name, '..', 'data', 'en_train',  'EI-reg-en_fear_train.txt')\n",
    "df_fear = pd.read_csv(train_file_name, header=None, delimiter=delimiter)\n",
    "fear_texts = get_all_tweets_as_whole_text(df_fear)\n",
    "fear_hashtags = [each[0] for each in Counter(re.findall('#\\w+', fear_texts)).most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = os.path.join(dir_name, '..', 'data', 'en_train',  'EI-reg-en_sadness_train.txt')\n",
    "df_sadness = pd.read_csv(train_file_name, header=None, delimiter=delimiter)\n",
    "sadness_texts = get_all_tweets_as_whole_text(df_sadness)\n",
    "sadness_hashtags = [each[0] for each in Counter(re.findall('#\\w+', sadness_texts)).most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = os.path.join(dir_name, '..', 'data', 'en_train',  'EI-reg-en_joy_train.txt')\n",
    "df_joy = pd.read_csv(train_file_name, header=None, delimiter=delimiter)\n",
    "joy_texts = get_all_tweets_as_whole_text(df_joy)\n",
    "joy_hashtags = [each[0] for each in Counter(re.findall('#\\w+', joy_texts)).most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = anger_hashtags + joy_hashtags + sadness_hashtags + fear_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_hashtags = [each[0] for each in Counter(hashtags).most_common() if each[1]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anger : 483\n",
      "fear : 963\n",
      "joy : 529\n",
      "sadness : 582\n",
      "unique : 2557\n",
      "total : 3363\n"
     ]
    }
   ],
   "source": [
    "anger_hashtags = [each for each in anger_hashtags if each in unique_hashtags]\n",
    "fear_hashtags = [each for each in fear_hashtags if each in unique_hashtags]\n",
    "joy_hashtags = [each for each in joy_hashtags if each in unique_hashtags]\n",
    "sadness_hashtags = [each for each in sadness_hashtags if each in unique_hashtags]\n",
    "print('Anger : '+str(len(anger_hashtags)))\n",
    "print('fear : '+str(len(fear_hashtags)))\n",
    "print('joy : '+str(len(joy_hashtags)))\n",
    "print('sadness : '+str(len(sadness_hashtags)))\n",
    "print('unique : '+str(len(unique_hashtags)))\n",
    "print('total : '+str(len(hashtags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode Emo Hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1371909298\n",
      "          0                 1         2\n",
      "3908   fear         #westbank  1.951954\n",
      "3909   fear     #apprehension  1.951954\n",
      "3910   fear            #su4mh  1.951954\n",
      "3911   fear          aaaaaaah  1.951954\n",
      "3912   fear              #ied  1.951954\n",
      "3913   fear        #coldsweat  1.951954\n",
      "3914   fear             #isaf  1.951954\n",
      "3915   fear           cryotek  1.951954\n",
      "3916   fear          #rushing  1.951954\n",
      "3917   fear              #shy  1.951954\n",
      "3918   fear     #apprehensive  1.951954\n",
      "3919   fear            #mosul  1.951954\n",
      "3920   fear          #fearful  1.951954\n",
      "3921   fear           #ashdod  1.951954\n",
      "3922   fear    #socialanxiety  1.951954\n",
      "3923   fear        #backtrack  1.951954\n",
      "3924   fear         terrifies  1.951954\n",
      "3925   fear   #claustrophobia  1.951954\n",
      "3926   fear            qassam  1.951954\n",
      "3927   fear        #hezbollah  1.951954\n",
      "3928   fear          #hamas25  1.951954\n",
      "3929   fear              #dfw  1.951954\n",
      "3930   fear           #feared  1.951954\n",
      "3931   fear          #taliban  1.951954\n",
      "3932   fear       rationality  1.951954\n",
      "3933   fear  #israelunderfire  1.951954\n",
      "3934   fear     #ihatespiders  1.951954\n",
      "3935   fear         #pharmacy  1.951954\n",
      "3936   fear              #pij  1.951954\n",
      "3937   fear          #conquer  1.951954\n",
      "...     ...               ...       ...\n",
      "26997   joy           killers  0.008800\n",
      "26998   joy             psych  0.008800\n",
      "26999   joy           atlanta  0.008800\n",
      "27000   joy              laid  0.008800\n",
      "27001   joy            papers  0.008800\n",
      "27002   joy          showered  0.008800\n",
      "27003   joy              till  0.008547\n",
      "27004   joy             faces  0.006399\n",
      "27005   joy                24  0.006239\n",
      "27006   joy         finishing  0.006151\n",
      "27007   joy             ghost  0.005845\n",
      "27008   joy         therefore  0.005845\n",
      "27009   joy             films  0.005461\n",
      "27010   joy          survived  0.004961\n",
      "27011   joy            quotes  0.004961\n",
      "27012   joy               toy  0.004285\n",
      "27013   joy              safe  0.003569\n",
      "27014   joy        wallflower  0.003320\n",
      "27015   joy          monsters  0.003320\n",
      "27016   joy             frame  0.003320\n",
      "27017   joy          romantic  0.003320\n",
      "27018   joy           signing  0.003320\n",
      "27019   joy             pulls  0.003320\n",
      "27020   joy               100  0.003320\n",
      "27021   joy               dvd  0.003320\n",
      "27022   joy            sweets  0.003320\n",
      "27023   joy             apart  0.002558\n",
      "27024   joy            doctor  0.002035\n",
      "27025   joy              grad  0.001137\n",
      "27026   joy              1000  0.001137\n",
      "\n",
      "[15421 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "file_name = os.path.join(dir_name, '..','resources', 'NRC-Hashtag-Emotion-Lexicon-v0.2', 'NRC-Hashtag-Emotion-Lexicon-v0.2.txt')\n",
    "df_emo_hashtag = pd.read_csv(file_name, header=None, delimiter=delimiter)\n",
    "df_emo_hashtag = df_emo_hashtag[(df_emo_hashtag[0] == 'sadness') |\n",
    "                               (df_emo_hashtag[0] == 'anger') | \n",
    "                               (df_emo_hashtag[0] == 'fear') | \n",
    "                               (df_emo_hashtag[0] == 'joy')]\n",
    "print(df_emo_hashtag[(df_emo_hashtag[0] == 'anger') & (df_emo_hashtag[1] == 'pissed')][2].values[0])\n",
    "print(df_emo_hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['fear', '#westbank', 1.95195360556393],\n",
       "       ['fear', '#apprehension', 1.95195360556393],\n",
       "       ['fear', '#su4mh', 1.95195360556393],\n",
       "       ..., \n",
       "       ['joy', 'doctor', 0.0020351827030041197],\n",
       "       ['joy', 'grad', 0.00113671004597709],\n",
       "       ['joy', '1000', 0.00113671004597709]], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_hashtag = df_emo_hashtag.as_matrix()\n",
    "emo_hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy', 'grad', 0.00113671004597709], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_emo_hashtag(emotion, hashtag):\n",
    "    temp = None\n",
    "    for each in emo_hashtag: \n",
    "        if each[0] == emotion and (each[1] == hashtag):\n",
    "            if temp is None:\n",
    "                temp = each\n",
    "            else:\n",
    "                temp = np.concatenate((temp, each), axis=0)\n",
    "    return temp\n",
    "filter_emo_hashtag('joy', 'grad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(tweet_content):\n",
    "    return re.findall('#[a-zA-Z]+', tweet_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtag_emo_value(hashtag, emotion):\n",
    "    emo_value = filter_emo_hashtag(emotion, hashtag)\n",
    "    return emo_value[2] if emo_value is not None else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emot_value_from_hashtag(tweet_content, emotion):\n",
    "    hashtags = get_hashtags(tweet_content)\n",
    "    if len(hashtags) > 0:\n",
    "        _t_emo = []\n",
    "        for each_hashtag in hashtags:\n",
    "            _t_emo.append(get_hashtag_emo_value(each_hashtag, emotion))\n",
    "    else:\n",
    "        _t_emo = [0]\n",
    "    return np.mean(_t_emo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Unicode Emoticon Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(dir_name, '..','resources', 'NRC-Emoticon-Lexicon-v1.0', 'Emoticon-unigrams.txt')\n",
    "emoticon_lexicon_unigram_df = pd.read_csv(file_name, header=None, delimiter=delimiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(dir_name, '..','resources', 'NRC-Emoticon-Lexicon-v1.0', 'Emoticon-bigrams.txt')\n",
    "emoticon_lexicon_bigram_df = pd.read_csv(file_name, header=None, delimiter=delimiter, quoting=csv.QUOTE_NONE, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(each_value, n_gram_value=1):\n",
    "    def get_tuple(current_tuple):\n",
    "        n_gram_value = str()\n",
    "        for each in current_tuple:\n",
    "            n_gram_value = str(each) if len(n_gram_value) == 0 else str(n_gram_value) + ' ' + str(each)\n",
    "        return n_gram_value\n",
    "    \n",
    "    # if each_value is list convert it into type str\n",
    "    if type(each_value) is list:\n",
    "        _t = ''\n",
    "        for each_ in each_value:\n",
    "            _t += each_\n",
    "        each_value = _t\n",
    "    \n",
    "    return [get_tuple(each) for each in ngrams(each_value.split(), n_gram_value)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emoticon_ngram_value(df, df_emoticon_lexicon, n_gram_value=1):\n",
    "    df = df[1].str.replace(r\"([a-zA-Z]+)([\" + string.punctuation + ']+[^a-zA-Z])', '\\\\1 \\\\2').str.strip().str.lower()\n",
    "    df = df.apply(lambda each_value: get_ngrams(each_value, n_gram_value))\n",
    "    lexicon_scores_list = []\n",
    "    for each_tweet in df:\n",
    "        word_count = 0\n",
    "        lexicon_score = 0\n",
    "        for each_word in each_tweet:\n",
    "            current_series = df_emoticon_lexicon[(df_emoticon_lexicon[0] == each_word)]\n",
    "            if not current_series.empty: #Is empty\n",
    "                lexicon_score += current_series.iloc[0][1]\n",
    "                word_count +=1\n",
    "        lexicon_score = 0 if word_count==0 \\\n",
    "            else (lexicon_score) / (word_count)\n",
    "        lexicon_scores_list.append(lexicon_score)\n",
    "    return np.array(lexicon_scores_list).reshape(len(lexicon_scores_list), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Vector (by averaging the constituent word vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data file... Please wait...\n",
      "Successfully loaded 3.6 G bin file!\n"
     ]
    }
   ],
   "source": [
    "def load_word_vectors():\n",
    "    global dir_name\n",
    "    embedding_file_loc = os.path.join(dir_name, '..', 'resources', 'word2vec','GoogleNews-vectors-negative300.bin')\n",
    "#     embedding_file_loc = os.path.join(dir_name, '..', 'resources', 'glove.6B', 'glove.')\n",
    "    print(\"Loading the data file... Please wait...\")\n",
    "    word_model = gensim.models.KeyedVectors.load_word2vec_format(embedding_file_loc, binary=True)\n",
    "    print(\"Successfully loaded 3.6 G bin file!\")\n",
    "    return word_model\n",
    "word_model = load_word_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase_vector_obj(value):\n",
    "    return PhraseVector(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhraseVector:\n",
    "    def __init__(self, phrase):\n",
    "        self.phrase = phrase\n",
    "        self.vector = self.phrase_to_vec(phrase)\n",
    "        self.pos_tag = self.get_words_in_phrase(phrase)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_vector_set_to_average(vector_set, ignore=[]):\n",
    "        if len(ignore) == 0:\n",
    "            return np.mean(vector_set, axis=0)\n",
    "        else:\n",
    "            return np.dot(np.transpose(vector_set), ignore) / sum(ignore)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_unique_token_tags(vector1, vector2):\n",
    "        tag_list = []\n",
    "        for each_tag in vector1.pos_tag + vector2.pos_tag:\n",
    "            if each_tag not in tag_list:\n",
    "                tag_list.append(each_tag)\n",
    "        return tag_list\n",
    "\n",
    "    def phrase_to_vec(self, phrase):\n",
    "        # _stop_words = stopwords.words(\"english\")\n",
    "        phrase = phrase.lower()\n",
    "        verified_words = [word for word in phrase.split()]\n",
    "        vector_set = []\n",
    "        for each_word in verified_words:\n",
    "            try:\n",
    "                word_vector = word_model[each_word]\n",
    "                vector_set.append(word_vector)\n",
    "            except:\n",
    "                pass\n",
    "        return self.convert_vector_set_to_average(vector_set)\n",
    "\n",
    "    def get_cosine_similarity(self, other_vector):\n",
    "        cosine_similarity = np.dot(self.vector, other_vector.vector) / (\n",
    "        np.linalg.norm(self.vector) * np.linalg.norm(other_vector.vector))\n",
    "        try:\n",
    "            if math.isnan(cosine_similarity):\n",
    "                cosine_similarity = 0\n",
    "        except:\n",
    "            cosine_similarity = 0\n",
    "        return cosine_similarity\n",
    "\n",
    "    def get_words_in_phrase(self, phrase):\n",
    "        if phrase.strip() == '':\n",
    "            return []\n",
    "        else:\n",
    "            tagged_input = nltk.pos_tag(phrase.split(), tagset='universal')\n",
    "            prev_item, prev_tag = tagged_input[0]\n",
    "            g_item_list = [prev_item]\n",
    "            cur_group_index = 0\n",
    "            space = ' '\n",
    "            revised_tag = []\n",
    "            for cur_item, cur_tag in tagged_input[1:]:\n",
    "                cur_item = cur_item.lower()\n",
    "                if prev_tag is cur_tag:\n",
    "                    g_item_list[cur_group_index] += space + cur_item\n",
    "                else:\n",
    "                    revised_tag.append((g_item_list[cur_group_index], prev_tag))\n",
    "                    prev_tag = cur_tag\n",
    "                    g_item_list.append(cur_item)\n",
    "                    cur_group_index += 1\n",
    "            revised_tag.append((g_item_list[cur_group_index], prev_tag))\n",
    "            return revised_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase_vectors(df, emotion):\n",
    "    tweet_vectors_obj = None\n",
    "    tweet_vectors = None\n",
    "    labels = None\n",
    "\n",
    "    tweet_vectors_obj = np.vectorize(get_phrase_vector_obj)(df[1].values)\n",
    "    tweet_vectors = np.array([[]])\n",
    "    for each_vector in tweet_vectors_obj:\n",
    "        curr_vector = each_vector.vector\n",
    "        if np.isnan(curr_vector).any():\n",
    "            curr_vector = np.zeros(shape=(1, 300))\n",
    "        else:\n",
    "            curr_vector = curr_vector.reshape(1, len(each_vector.vector))\n",
    "        if np.min(tweet_vectors.shape) == 0:\n",
    "            tweet_vectors = np.concatenate((tweet_vectors, curr_vector), axis=1)\n",
    "        else:\n",
    "            tweet_vectors = np.concatenate((tweet_vectors, curr_vector), axis=0)\n",
    "    labels = df[3].values\n",
    "\n",
    "    return tweet_vectors, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarity and Subjectivity using Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polarity_and_subjectivity(df, emotion):\n",
    "    polarity_list = []\n",
    "    subjectivity_list = []\n",
    "\n",
    "    polarity_list = np.array(list(map(lambda x: Sentence(x).polarity, df[1].values)))\n",
    "    subjectivity_list = np.array(list(map(lambda x: Sentence(x).subjectivity, df[1].values)))\n",
    "\n",
    "    polarity_list = polarity_list.reshape(len(polarity_list),1)\n",
    "    subjectivity_list = subjectivity_list.reshape(len(subjectivity_list),1)\n",
    "    return polarity_list, subjectivity_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concreteness Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concrete = pd.read_csv(os.path.join(dir_name, '..', 'resources', 'Concreteness_ratings_Brysbaert_et_al_BRM.txt')\n",
    "                 , delimiter=delimiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concreteness_scores(df, df_concrete):\n",
    "    # translation_table = dict.fromkeys(map(ord, string.punctuation), None)\n",
    "    df = df[1].str.lower().str.split()\n",
    "    concrete_correctness_values = []\n",
    "    for each_tweet in df:\n",
    "        word_count = 0\n",
    "        current_correctness_score = 0\n",
    "        for each_word in each_tweet:\n",
    "            current_series = df_concrete[['Word','Conc.M']][(df_concrete['Word']==each_word)]\n",
    "            if not current_series.empty: #Is empty\n",
    "                current_correctness_score += current_series.iloc[0][1]\n",
    "                word_count +=1\n",
    "        current_correctness_score = 0 if word_count==0 \\\n",
    "            else (current_correctness_score) / (word_count)\n",
    "        concrete_correctness_values.append(current_correctness_score)\n",
    "    return np.array(concrete_correctness_values).reshape(len(concrete_correctness_values), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NRC-Emoticon-AffLexNegLex-v1.0 - Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_file_name = os.path.join(dir_name, '..', 'resources', 'NRC-Emoticon-AffLexNegLex-v1.0', 'Emoticon-AFFLEX-NEGLEX-unigrams.txt')\n",
    "df_emoticon_affneg_uni = pd.read_csv(resource_file_name, header=None, delimiter=delimiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_file_name = os.path.join(dir_name, '..', 'resources', 'NRC-Emoticon-AffLexNegLex-v1.0', 'Emoticon-AFFLEX-NEGLEX-bigrams.txt')\n",
    "df_emoticon_affneg_bi = pd.read_csv(resource_file_name, header=None, delimiter=delimiter, quoting=csv.QUOTE_NONE, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_emoticon_ngram_value()` function is re-used for calculating the emoticon-affLexNegLex features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NRC-Hashtag-Sentiment-AffLexNegLex-v1.0 - Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_file_name = os.path.join(dir_name, '..', 'resources', 'NRC-Hashtag-Sentiment-AffLexNegLex-v1.0', 'HS-AFFLEX-NEGLEX-unigrams.txt')\n",
    "df_nrchashtag_afflexneglex_unigrams = pd.read_csv(resource_file_name, header=None, delimiter=delimiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_hashtag_afflexneglex(df, df_nrchashtag_afflexneglex_unigrams):\n",
    "    hashtag_afflexneg_uni_features = []\n",
    "    df = df[1].str.findall('#\\w+').values\n",
    "    for each_tweet in df:\n",
    "        word_count = 0\n",
    "        each_value = 0\n",
    "        for each_hashtag in each_tweet:\n",
    "            current_series = df_nrchashtag_afflexneglex_unigrams[(df_nrchashtag_afflexneglex_unigrams[0])==each_hashtag]\n",
    "            if not current_series.empty:\n",
    "                each_value += current_series[1].values[0]\n",
    "                word_count += 1\n",
    "        each_value = 0 if word_count == 0 else (each_value)/(word_count)\n",
    "        hashtag_afflexneg_uni_features.append(each_value)\n",
    "    return np.array(hashtag_afflexneg_uni_features).reshape(len(hashtag_afflexneg_uni_features), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NRC-Hashtag-Sentiment-AffLexNegLex - Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`def get_emoticon_afflex_bigram_value(df, df_emoticon_lexicon)` may contain some errors by exclusion. The regular expression used has not vigorously been tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(dir_name, '..','resources', 'NRC-Hashtag-Sentiment-AffLexNegLex-v1.0', 'HS-AFFLEX-NEGLEX-bigrams.txt')\n",
    "df_hashtag_senti_afflex_bi = pd.read_csv(file_name, header=None, delimiter=delimiter, quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "df_hashtag_senti_afflex_bi[0]  = df_hashtag_senti_afflex_bi[0].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emoticon_afflex_bigram_value(df, df_emoticon_lexicon):\n",
    "    df = \\\n",
    "        df[1].str.replace(r\"([a-zA-Z]+)([^\\w\\s]+)([^a-zA-Z])\", '\\\\1 \\\\2 \\\\3') \\\n",
    "            .str.strip() \\\n",
    "            .str.lower() \\\n",
    "            .str.findall(r'[^\\w\\s]+(?:\\s*#[a-zA-Z]+)+(?:\\s*[^\\w\\s]+)?')\n",
    "    df = df.apply(lambda each_value: get_ngrams(each_value, 2))\n",
    "    lexicon_scores_list = []\n",
    "    for each_tweet in df:\n",
    "        word_count = 0\n",
    "        lexicon_score = 0\n",
    "        for each_word in each_tweet:\n",
    "            current_series = df_emoticon_lexicon[(df_emoticon_lexicon[0] == each_word)]\n",
    "            if not current_series.empty: #Is empty\n",
    "                lexicon_score += current_series.iloc[0][1]\n",
    "                word_count +=1\n",
    "        lexicon_score = 0 if word_count==0 \\\n",
    "            else (lexicon_score) / (word_count)\n",
    "        lexicon_scores_list.append(lexicon_score)\n",
    "    return np.array(lexicon_scores_list).reshape(len(lexicon_scores_list), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NRC Sentiment Hashtag Sentiment Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_unigram_hashtag_afflexneglex` is used for calculating the unigram features for this feature. \n",
    "\n",
    "`get_emoticon_afflex_bigram_value` is used to calculate the bigram value for this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(dir_name, '..','resources', 'NRC-Hashtag-Sentiment-Lexicon-v1.0', 'HS-unigrams.txt')\n",
    "df_hashtag_senti_uni = pd.read_csv(file_name, header=None, delimiter=delimiter, quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "df_hashtag_senti_uni[0]  = df_hashtag_senti_uni[0].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(dir_name, '..','resources', 'NRC-Hashtag-Sentiment-Lexicon-v1.0', 'HS-bigrams.txt')\n",
    "df_hashtag_senti_bi = pd.read_csv(file_name, header=None, delimiter=delimiter, quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "df_hashtag_senti_bi[0]  = df_hashtag_senti_bi[0].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anew Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anew = pd.read_csv(\n",
    "    os.path.join(dir_name, '..', 'resources', 'Warriner, Kuperman, Brysbaert - 2013 BRM-ANEW expanded.tsv'),\n",
    "    delimiter=delimiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anew_scores(df, df_anew):\n",
    "    # translation_table = dict.fromkeys(map(ord, string.punctuation), None)\n",
    "    df = df[1].str.lower().str.split()\n",
    "    anew_scores_list = np.array([[]])\n",
    "    for each_tweet in df:\n",
    "        word_count = 0\n",
    "        current_anew_scores = np.zeros(3)\n",
    "        for each_word in each_tweet:\n",
    "            current_series = df_anew[['Word', 'V.Mean.Sum', 'A.Mean.Sum', 'D.Mean.Sum']][(df_anew['Word'] == each_word)]\n",
    "            if not current_series.empty:\n",
    "                current_anew_scores = np.add(current_anew_scores, current_series.iloc[0][1:].values)\n",
    "                word_count +=1\n",
    "        current_anew_scores = np.zeros(3) if word_count==0 \\\n",
    "            else (current_anew_scores) / (word_count)\n",
    "        current_anew_scores = current_anew_scores.reshape(1, len(current_anew_scores))\n",
    "        if np.min(anew_scores_list.shape) == 0:\n",
    "            anew_scores_list = np.concatenate((anew_scores_list, current_anew_scores), axis=1)\n",
    "        else:\n",
    "            anew_scores_list = np.concatenate((anew_scores_list, current_anew_scores), axis=0)\n",
    "    return anew_scores_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Features Concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(df, emotion):\n",
    "    global df_emoticon_affneg_uni\n",
    "    global df_emoticon_affneg_bi\n",
    "    global df_nrchashtag_afflexneglex_unigrams\n",
    "    global df_hashtag_senti_afflex_bi\n",
    "    global df_concrete\n",
    "    global df_anew\n",
    "    global emoticon_lexicon_unigram_df\n",
    "    global emoticon_lexicon_bigram_df\n",
    "    global df_hashtag_senti_uni\n",
    "    global df_hashtag_senti_bi\n",
    "    polarity_list, subjectivity_list = get_polarity_and_subjectivity(df, emotion)\n",
    "    tweet_vectors, labels = get_phrase_vectors(df, emotion)\n",
    "    emoticon_lexicon_unigram_value = get_emoticon_ngram_value(df, emoticon_lexicon_unigram_df, n_gram_value=1)\n",
    "    emoticon_lexicon_bigram_value = get_emoticon_ngram_value(df, emoticon_lexicon_bigram_df, n_gram_value=2)\n",
    "    hashtag_feature_nrc_hashtag_emoticon = np.array([get_emot_value_from_hashtag(each, emotion) \n",
    "                                                       for each in df.as_matrix()[:,1]]).reshape(len(df), 1)\n",
    "    nrc_emoticon_afflexneglex_unigrams = get_emoticon_ngram_value(df, df_emoticon_affneg_uni, n_gram_value=1)\n",
    "    nrc_emoticon_afflexneglex_unigrams = get_emoticon_ngram_value(df, df_emoticon_affneg_bi, n_gram_value=2)\n",
    "    hashtag_afflexneg_uni_features = get_unigram_hashtag_afflexneglex(df, df_nrchashtag_afflexneglex_unigrams)\n",
    "    hashtag_afflexneg_bi_features = get_unigram_hashtag_afflexneglex(df, df_nrchashtag_afflexneglex_unigrams)\n",
    "    concreteness_features = get_concreteness_scores(df, df_concrete)\n",
    "    anew_features = get_anew_scores(df, df_anew)\n",
    "    sentiment_lexicon_features_uni = get_unigram_hashtag_afflexneglex(df, df_hashtag_senti_uni)\n",
    "    sentiment_lexicon_features_bi = get_emoticon_afflex_bigram_value(df, df_hashtag_senti_bi)\n",
    "    return np.concatenate((polarity_list, \n",
    "                           subjectivity_list, \n",
    "                           tweet_vectors, \n",
    "                           emoticon_lexicon_unigram_value,\n",
    "                           emoticon_lexicon_bigram_value,\n",
    "                           hashtag_feature_nrc_hashtag_emoticon,\n",
    "                           nrc_emoticon_afflexneglex_unigrams, \n",
    "                           hashtag_afflexneg_uni_features,\n",
    "                           hashtag_afflexneg_bi_features,\n",
    "                           concreteness_features, \n",
    "                           anew_features), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_ndarray_from_df(df):\n",
    "    translation_table = dict.fromkeys(map(ord, string.punctuation), None)\n",
    "    return df[1].str.lower().str.translate(translation_table).str.split().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.constraints import min_max_norm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sys\n",
    "\n",
    "max_length = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_parameters(features):\n",
    "    vocab_size, embedded_vector_length = features.shape\n",
    "    max_length = vocab_size\n",
    "    embedding_matrix = features\n",
    "    return vocab_size, embedded_vector_length, max_length, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_model(vocab_size, embedded_vector_length, embedding_matrix, max_length, \n",
    "                  optimizer='adam',\n",
    "                  loss='mean_squared_logarithmic_error'):\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size, embedded_vector_length, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "    model.add(e)\n",
    "    lstm = Bidirectional(LSTM(300, activation='relu',\n",
    "                              kernel_initializer='random_uniform',\n",
    "                              bias_initializer='zeros',\n",
    "                              kernel_constraint=min_max_norm(min_value=-1.0, max_value=1.0, rate=0.5, axis=0),\n",
    "                              bias_constraint=min_max_norm(min_value=-1.0, max_value=1.0, rate=0.5, axis=0)))\n",
    "    model.add(lstm)\n",
    "    model.add(Dense(300, activation='relu'))\n",
    "#     # compile the model\n",
    "#     model.compile(optimizer=optimizer, loss=loss)\n",
    "#     # summarize the model\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense_model(vocab_size, embedded_vector_length, embedding_matrix, max_length, \n",
    "                  optimizer='adam',\n",
    "                  loss='mean_squared_logarithmic_error'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(300, \n",
    "                    activation='relu', \n",
    "                    input_shape=(embedded_vector_length,)))\n",
    "#     # compile the model\n",
    "#     model.compile(optimizer=optimizer, loss=loss)\n",
    "#     # summarize the model\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnn_model(embedding_models,\n",
    "                  optimizer='adam',\n",
    "                  loss='mean_squared_logarithmic_error',\n",
    "                  output_activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Merge(embedding_models, mode='concat', concat_axis=1))\n",
    "    model.add(Dense(1, activation=output_activation,\n",
    "                    kernel_initializer='random_uniform',\n",
    "                    bias_initializer='zeros',\n",
    "                    kernel_constraint=min_max_norm(min_value=-1.0, max_value=1.0, rate=.5, axis=0),\n",
    "                    bias_constraint=min_max_norm(min_value=-1.0, max_value=1.0, rate=0.5, axis=0)))\n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "    return model\n",
    "    # fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(tweet_ids, assgn_emotions, tweet_contents, predicted_scores, file_name):\n",
    "    global dir_name\n",
    "    if not os.path.exists(os.path.join(dir_name, '..', 'output')):\n",
    "        os.makedirs(os.path.join(dir_name, '..', 'output'))\n",
    "    with open(os.path.join(dir_name, '..', 'output', file_name), 'w') as f:\n",
    "        file_writer = csv.writer(f, delimiter='\\t')\n",
    "        for each_tweet_id, each_tweet_content, each_emotion, each_score in \\\n",
    "                zip(tweet_ids, tweet_contents, assgn_emotions, predicted_scores):\n",
    "            file_writer.writerow([each_tweet_id, each_tweet_content, each_emotion, each_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(df, embeddings_index=None):\n",
    "    cleaned_tokenized = get_tokenized_ndarray_from_df(df)\n",
    "    unique_tokens = set()\n",
    "    for each_tweet in cleaned_tokenized:\n",
    "        filtered_set = []\n",
    "        if embeddings_index is None:\n",
    "            filtered_set = each_tweet\n",
    "        else:\n",
    "            for each_word in each_tweet:\n",
    "                if each_word in embeddings_index.vocab.keys():\n",
    "                    filtered_set.append(each_word)\n",
    "        unique_tokens = unique_tokens.union(set(filtered_set))\n",
    "    unique_tokens = list(unique_tokens)\n",
    "    vocab = {each_word : each_index+1 for each_index, each_word in enumerate(unique_tokens)}\n",
    "    vocab['<unk>' ] = 0\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(vocab, embeddings_index, embedded_vector_length=300):\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = zeros((vocab_size, embedded_vector_length))\n",
    "    for word in vocab.keys():\n",
    "        if word in embeddings_index.vocab.keys():\n",
    "            embedding_matrix[vocab[word]] = embeddings_index.syn0[embeddings_index.vocab[word].index]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encode_docs(df, vocab):\n",
    "    cleaned_tokenized = get_tokenized_ndarray_from_df(df)\n",
    "    encoded_docs = []\n",
    "    max_length = -1\n",
    "    for each_tweet in cleaned_tokenized:\n",
    "        encoded_tokenized_tweet = []\n",
    "        for each_word in each_tweet:\n",
    "            if each_word in vocab.keys():\n",
    "                encoded_tokenized_tweet.append(vocab[each_word])\n",
    "            else:\n",
    "                encoded_tokenized_tweet.append(0)\n",
    "        current_length = len(encoded_tokenized_tweet)\n",
    "        if current_length > max_length:\n",
    "            max_length = current_length\n",
    "        encoded_docs.append(encoded_tokenized_tweet)\n",
    "    return encoded_docs, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_docs(df, vocab, encoded_size=None):\n",
    "    encoded_docs, max_length = get_encode_docs(df, vocab)\n",
    "    padded_docs = np.array([[]])\n",
    "    if encoded_size is not None:\n",
    "        max_length = encoded_size\n",
    "    for each_encoded_doc in encoded_docs:\n",
    "        each_encoded_size = len(each_encoded_doc)\n",
    "        each_encoded_doc = np.array(each_encoded_doc)\n",
    "        if max_length >= each_encoded_size: \n",
    "            each_encoded_doc = np.array([np.pad(each_encoded_doc, (0, max_length-each_encoded_size), 'constant')])\n",
    "        else:\n",
    "            each_encoded_doc = np.array([each_encoded_doc[:max_length]])\n",
    "        if padded_docs.size == 0:\n",
    "            padded_docs = np.concatenate((padded_docs, each_encoded_doc), axis=1)\n",
    "        else:\n",
    "            padded_docs = np.concatenate((padded_docs, each_encoded_doc), axis=0)\n",
    "    return padded_docs, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built vocabulary...\n",
      "Padded docs for training is created...\n",
      "Loaded embedding matrix...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 33, 300)           1249200   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 600)               1442400   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 300)               180300    \n",
      "=================================================================\n",
      "Total params: 2,871,900\n",
      "Trainable params: 1,622,700\n",
      "Non-trainable params: 1,249,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "char_level = False\n",
    "docs = []\n",
    "labels = []\n",
    "tweet_ids = []\n",
    "emotions = []\n",
    "\n",
    "embedded_vector_length = 300\n",
    "emotion_names = ['sadness', 'joy', 'anger', 'fear']\n",
    "embedding_name = 'word2vec'\n",
    "embeddings_index = word_model\n",
    "\n",
    "for emot_id, emotion in enumerate(emotion_names):\n",
    "    training_file_name = os.path.join(dir_name, '..','data','en_train','EI-reg-en_'+emotion+'_train.txt')\n",
    "    df = pd.read_csv(training_file_name, header=None, delimiter=delimiter)\n",
    "    tweet_ids_train, docs_train, emotions_train, label_train = [df[each].values for each in range(4)]\n",
    "\n",
    "#     print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    vocab = build_vocab(df, embeddings_index)\n",
    "    print('Built vocabulary...')\n",
    "    padded_docs_train, em_max_length = get_padded_docs(df, vocab)\n",
    "    print('Padded docs for training is created...')\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = build_embedding_matrix(vocab, embeddings_index)\n",
    "    print('Loaded embedding matrix...')\n",
    "    # define model with word embeddings\n",
    "    _, embedded_vector_length = embedding_matrix.shape\n",
    "    embedding_model = get_embedding_model(vocab_size, embedded_vector_length, embedding_matrix, em_max_length)\n",
    "    # Added features\n",
    "    current_feature = get_features(df, emotion)\n",
    "    print('extra features loaded...')\n",
    "    print('loaded word model...')\n",
    "    vocab_size, embedded_vector_length, max_length, embedding_matrix = get_embedding_parameters(current_feature)\n",
    "\n",
    "    phrase_vector_model = get_dense_model(vocab_size, embedded_vector_length, embedding_matrix, max_length)\n",
    "    print('loaded phrase model...')\n",
    "    model=get_rnn_model([embedding_model, phrase_vector_model])\n",
    "    \n",
    "    print(padded_docs_train.shape)\n",
    "    print(current_feature.shape)\n",
    "    \n",
    "    model.fit([padded_docs_train, current_feature], label_train, verbose=1)\n",
    "    \n",
    "    print('model has been fit...')\n",
    "    # dev set\n",
    "    dev_file_name=os.path.join(dir_name, '..', 'data', 'en_dev', '2018-EI-reg-En-' + emotion + '-dev.txt')\n",
    "    df = pd.read_csv(dev_file_name, header=None, delimiter=delimiter)\n",
    "    tweet_ids, docs, emotions, labels = [df[each].values for each in range(4)]\n",
    "    padded_docs_dev, _ = get_padded_docs(df, vocab, encoded_size = em_max_length)\n",
    "    dev_features = get_features(df, emotion)\n",
    "    np.savetxt(emotion+'_validation.tsv', dev_features, delimiter='\\t')\n",
    "    \n",
    "    predicted_list = model.predict([padded_docs_dev, dev_features])\n",
    "    write_to_file(tweet_ids, emotions, docs, labels, emotion + '_' + embedding_name + '_dev_labels')\n",
    "    predicted_list = [each[0] for each in predicted_list]\n",
    "    write_to_file(tweet_ids, emotions, docs, predicted_list, emotion+'_'+embedding_name+'_dev')\n",
    "\n",
    "    print('Mean Squared Error of Validation Set: '+str(mean_squared_error(labels, predicted_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
